import re

import scrapy

from goto_eat_scrapy.items import ShopItem
from goto_eat_scrapy.spiders.abstract import AbstractSpider


class YamaguchiSpider(AbstractSpider):
    """
    usage:
      $ scrapy crawl yamaguchi -O yamaguchi.csv
    """

    name = "yamaguchi"
    allowed_domains = ["gotoeat-yamaguchi.com"]

    def start_requests(self):
        area_list = [
            {"code": "01-shop-area", "name": "å²©å›½ã‚¨ãƒªã‚¢"},
            {"code": "02-shop-area", "name": "æŸ³äº•ã‚¨ãƒªã‚¢"},
            {"code": "03-shop-area", "name": "å‘¨å—ã‚¨ãƒªã‚¢"},
            {"code": "04-shop-area", "name": "å±±å£ãƒ»é˜²åºœã‚¨ãƒªã‚¢"},
            {"code": "05-shop-area", "name": "è©ã‚¨ãƒªã‚¢"},
            {"code": "06-shop-area", "name": "é•·é–€ã‚¨ãƒªã‚¢"},
            {"code": "07-shop-area", "name": "å®‡éƒ¨ãƒ»å°é‡ç”°ãƒ»ç¾ç¥¢ã‚¨ãƒªã‚¢"},
            {"code": "08-shop-area", "name": "ä¸‹é–¢ã‚¨ãƒªã‚¢"},
        ]
        for area in area_list:
            url = "https://gotoeat-yamaguchi.com/use/?post_type=post&s=&cat_area%5B%5D={}".format(area["code"])
            yield scrapy.Request(url, callback=self.parse, meta={"area_name": area["name"]})

    def parse(self, response):
        self.logzero_logger.info(f"ğŸ’¾ url = {response.request.url}")

        area_name = response.meta["area_name"]
        for article in response.xpath('//ul[@id="shop-list"]/li'):
            item = ShopItem()
            item["shop_name"] = article.xpath('.//div[@class="left"]/h3/a/text()').get().strip()
            item["area_name"] = area_name

            genres = article.xpath('.//div[@class="left"]/p[@class="type"]/a/text()').getall()
            item["genre_name"] = "|".join([g.replace("â—", "") for g in genres])  # è¤‡æ•°ã‚¸ãƒ£ãƒ³ãƒ«ã‚ã‚Š

            item["address"] = (
                article.xpath('.//div[@class="left break"]/p/strong[contains(text(), "ï¼»ä½æ‰€ï¼½")]/../text()').get().strip()
            )
            item["opening_hours"] = (
                article.xpath('.//div[@class="left break"]/p/strong[contains(text(), "ï¼»å–¶æ¥­æ™‚é–“ï¼½")]/../text()')
                .get()
                .strip()
            )
            item["closing_day"] = (
                article.xpath('.//div[@class="left break"]/p/strong[contains(text(), "ï¼»å®šä¼‘æ—¥ï¼½")]/../text()').get().strip()
            )
            item["tel"] = article.xpath(
                './/div[@class="left break"]/p/strong[contains(text(), "ï¼»TELï¼½")]/../text()'
            ).get()

            # MEMO: å±±å£çœŒã®"rink"ã¯è¤‡æ•°æŒ‡å®šã§ãã€å…¬å¼HPä»¥å¤–ã ã‘ã§ãªãã€å„ç¨®SNSã‚¢ã‚«ã‚¦ãƒ³ãƒˆç­‰ã‚‚ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ãŒã€ã¨ã‚Šã‚ãˆãšå…ˆé ­ã®ã‚‚ã®ã ã‘ã‚’å–å¾—ã—ã¦ã„ã‚‹
            item["official_page"] = article.xpath('.//div[@class="rink"]/a[1]/@href').get()

            yield item

        # ã€Œ>ã€ãƒœã‚¿ãƒ³ãŒãªã‘ã‚Œã°(æœ€çµ‚ãƒšãƒ¼ã‚¸ãªã®ã§)çµ‚äº†
        next_page = response.xpath('//div[@role="navigation"]/a[@rel="next"]/@href').extract_first()
        if next_page is None:
            self.logzero_logger.info("ğŸ’» finished. last page = " + response.request.url)
            return

        self.logzero_logger.info(f"ğŸ›« next url = {next_page}")
        yield scrapy.Request(next_page, callback=self.parse, meta=response.meta)
