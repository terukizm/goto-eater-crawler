import demjson
import re
import scrapy
from goto_eat_scrapy.items import ShopItem
from goto_eat_scrapy.spiders.abstract import AbstractSpider

class MiyagiSpider(AbstractSpider):
    """
    usage:
      $ scrapy crawl miyagi -O miyagi.csv
    """
    name = 'miyagi'
    allowed_domains = [ 'gte-miyagi.jp' ]

    # @see https://gte-miyagi.jp/available.html
    # å®®åŸçœŒã¯ãƒšãƒ¼ã‚¸ãƒ³ã‚°ãªã—
    area_list = [
        {
            'url': 'https://gte-miyagi.jp/available_aobaku.php',
            'params': {'searchwords': ' ', 'area': 'ä»™å°å¸‚é’è‘‰åŒº', 'ch': 'all'},
        },
        {
            'url': 'https://gte-miyagi.jp/available_miyaginoku.php',
            'params': {'searchwords': ' ', 'area': 'ä»™å°å¸‚å®®åŸé‡åŒº', 'ch': 'all'},
        },
        {
            'url': 'https://gte-miyagi.jp/available_wakabayashiku.php',
            'params': {'searchwords': ' ', 'area': 'ä»™å°å¸‚è‹¥æ—åŒº', 'ch': 'all'},
        },
        {
            'url': 'https://gte-miyagi.jp/available_taihakuku.php',
            'params': {'searchwords': ' ', 'area': 'ä»™å°å¸‚å¤ªç™½åŒº', 'ch': 'all'},
        },
        {
            'url': 'https://gte-miyagi.jp/available_izumiku.php',
            'params': {'searchwords': ' ', 'area': 'ä»™å°å¸‚æ³‰åŒº', 'ch': 'all'},
        },
        {
            # å®®åŸçœŒåŒ—éƒ¨
            'url': 'https://gte-miyagi.jp/available03.php',
            'params': {'searchwords': ' ', 'area': 'all', 'ch': 'all'},
        },
        {
            # å®®åŸçœŒå—éƒ¨
            'url': 'https://gte-miyagi.jp/available04.php',
            'params': {'searchwords': ' ', 'area': 'all', 'ch': 'all'},
        }
    ]

    def start_requests(self):
        for row in self.area_list:
            url = row['url']
            params = row['params']
            self.logzero_logger.info(f'ğŸ’¾ params = {params}')
            yield scrapy.FormRequest(url, \
                    callback=self.parse, method='POST', \
                    formdata=params)

    def parse(self, response):
        # ã‚¨ãƒªã‚¢åå–å¾—
        text = response.xpath('//div[@class="wrap"]/div[@class="cont"]/h2/span/text()').extract_first()
        m = re.search(r'\[\s(?P<area_name>.*)\s\]', text)
        area_name = m.group('area_name')

        # ãƒãƒ¼ã‚«ãƒ¼è¡¨ç¤ºç”¨ã«ä½¿ã‚ã‚Œã¦ã„ã‚‹<script> ã‚¿ã‚°ä¸­ã®"const data = [ ... ];" ã‚’æŠ½å‡º
        # JSONã§ã¯ãªã„ã®ã§(javascript Objectå½¢å¼)ã€demjsonã§å¤‰æ›ã—ã¦åˆ©ç”¨
        text = response.xpath("//script[contains(., 'const data = [')]/text()").extract()[0]
        m = re.search(r'const data = (?P<js_data>\[.*?\]);', text, re.DOTALL)
        js_data_text = m.group('js_data')
        # MEMO: jsä¸­ã®ãƒ‡ãƒ¼ã‚¿ã¨çªãåˆã‚ã›ãŸã¨ãã€htmlã‚¿ã‚°ã®æœ‰ç„¡ã§ä¸€è‡´ã—ãªã„ã“ã¨ãŒã‚ã‚‹ã€‚
        # jsã®ä¸­èº«ã«ã¤ã„ã¦ã‚‚ShopItemã‚’é€šã—ãŸå½¢ã«å¤‰æ›ã—(ã“ã®ã¨ãã«HTMLã‚¿ã‚°ãŒstripã•ã‚Œã‚‹)ã€æ›¸å¼ã‚’çµ±ä¸€
        data_item_list = [ ShopItem(shop_name=row['name'].strip(), address=row['content'].strip(), provided_lat=row['lat'], provided_lng=row['lng']) \
            for row in (demjson.decode(js_data_text) if js_data_text else []) ]

        # å„åŠ ç›Ÿåº—æƒ…å ±ã‚’æŠ½å‡º
        self.logzero_logger.info(f'ğŸ’¾ url = {response.request.url}')
        for article in response.xpath('//div[@class="SLCont"]//dl[@class="shopList"]'):
            item = ShopItem()
            item['area_name'] = area_name
            item['shop_name'] = article.xpath('.//dt/text()').get().strip()
            item['genre_name'] = article.xpath('.//dd[1]/span[2]/text()').get().strip()

            # MEMO: ã‚¢ãƒ‘ãƒ´ã‚£ãƒ©ãƒ›ãƒ†ãƒ«ã®ä½æ‰€ã«<>ãŒå«ã¾ã‚Œã¦ãŠã‚Šã€ãã‚Œã‚’xpathã§å–å¾—ã—ãŸã¨ãã«ã‚¿ã‚°æ‰±ã„ã«ã•ã‚Œã¦ã—ã¾ã£ã¦(text()ã§å–ã‚Œãªã„)
            # <>å†…ãŒæŠœã‘è½ã¡ã‚‹ã€‚ãŸã ã€ãã‚‚ãã‚‚ä½æ‰€ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦<>ãŒå…¥ã£ã¦ãã¦ã‚‚ã€å‡ºåŠ›æ™‚ã«ã¡ã‚ƒã‚“ã¨&lt;&gt;ã«å¤‰æ›ã—ã¦å‡ºåŠ›ã™ã¹ããªã®ã§ã¯â€¦ ã¨ã„ã†æ°—æŒã¡ã‚‚â€¦
            place = ' '.join(article.xpath('.//dd[2]/span[2]/text()').getall())
            m = re.match(r'ã€’(?P<zip_code>.*?)\s(?P<address>.*)', place)
            item['address'] = m.group('address').strip()
            item['zip_code'] = m.group('zip_code').strip()
            item['tel'] = article.xpath('.//dd[3]/span[2]/text()').get().strip()

            # MEMO: æœ¬æ¥ã¯@hrefã§å–ã‚ŠãŸã„ãŒã€aãƒªãƒ³ã‚¯ãŒè²¼ã‚‰ã‚Œã¦ãªã„ã®ã‚‚ã‚ã‚‹ãŸã‚(2020/11/28)
            item['official_page'] = article.xpath('.//dd[4]/span[@class="url"]/text()').get()

            # jsã®nameè¦ç´ ãŒå®Œå…¨ä¸€è‡´ã€jsã®contentè¦ç´ ã«å«ã¾ã‚Œã‚‹ä½æ‰€ãŒéƒ¨åˆ†ä¸€è‡´ã™ã‚‹ã‚‚ã®ã‚’æŠ½å‡º
            match = [jsdata for jsdata in data_item_list \
                if (item['shop_name'] == jsdata['shop_name'] and (item['address'] in jsdata['address'])) ]

            if match:
                item['provided_lat'] = match[0]['provided_lat']
                item['provided_lng'] = match[0]['provided_lng']
            elif 1 < len(match):
                self.logzero_logger.warn(f'ğŸ”¥ å®®åŸçœŒã®latlngå–å¾—ã«ãŠã„ã¦ã€åº—åã¨ä½æ‰€ã§é‡è¤‡ã—ã¦ã„ã‚‹ã‚‚ã®ãŒã‚ã‚Šã¾ã—ãŸ: {match}')
            else:
                self.logzero_logger.warn(f'ğŸ”¥ å®®åŸçœŒã®latlngå–å¾—ã«ãŠã„ã¦ã€åº—åã¨ä½æ‰€ãŒä¸€è‡´ã—ãªã„ã‚‚ã®ãŒã‚ã‚Šã¾ã—ãŸ: item={item} @ã€Œ{area_name}ã€')

            yield item
