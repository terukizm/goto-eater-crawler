import re
import scrapy
import w3lib
import json
from goto_eat_scrapy.items import ShopItem
from goto_eat_scrapy.spiders.abstract import AbstractSpider

class TokyoSpider(AbstractSpider):
    """
    usage:
      $ scrapy crawl tokyo -O tokyo.csv
    """
    name = 'tokyo'
    allowed_domains = [ 'r.gnavi.co.jp' ]

    # ä¼æ¥­ã‚µã‚¤ãƒˆãªã®ã§(ãã‚Œã‚‚ã©ã†ã‹ã¨æ€ã†ãŒâ€¦) ä¸€å¿œæ°—ã‚’ä½¿ã†
    custom_settings = {
        'CONCURRENT_REQUESTS': 1,
        'CONCURRENT_REQUESTS_PER_DOMAIN': 1,
        'CONCURRENT_REQUESTS_PER_IP': 0,
        'DOWNLOAD_DELAY': 1,
        # MEMO: 16kä»¶ä»¥ä¸Šã‚ã‚‹ â†’ 11/28ã«è¦‹ãŸã‚‰23kä»¶è¶…ãˆã¦ãŸ... è©³ç´°ãƒšãƒ¼ã‚¸ã¾ã§è¦‹ãªã„ã¨ã„ã‘ãªã„ã®ã§ç§’é–“1ä»¶ã§è¨±ã—ã¦â€¦
    }

    start_urls = [
        # ç´™ã¨é›»å­ã€ä¸¡æ–¹ä½¿ãˆã‚‹åº—ã¨ã—ãŸ(ç´™ã—ã‹ä½¿ãˆãªã„åº—ã¯ã‚ã‚‹ãŒã€é›»å­ã—ã‹ä½¿ãˆãªã„åº—ã¯ãªã•ãã†)
        'https://r.gnavi.co.jp/area/tokyo/kods17214/rs/?gtet_all=1&resp=1&fwp=%E6%9D%B1%E4%BA%AC%E9%83%BD', # éƒ½å†…å…¨ä½“ã€é£Ÿäº‹åˆ¸å¯¾è±¡åº—(ã™ã¹ã¦)
    ]

    def parse(self, response):
        # å„åŠ ç›Ÿåº—æƒ…å ±ã‚’æŠ½å‡º
        self.logzero_logger.info(f'ğŸ’¾ url = {response.request.url}')
        for article in response.xpath('//div[@class="result-cassette__wrapper result-cassette__wrapper--normal"]/ul[@class="result-cassette__list"]/li'):
            url = article.xpath('.//div[@class="result-cassette__box"]//a[@class="result-cassette__box-title js-measure"]/@href').get()
            yield scrapy.Request(url, callback=self.detail)

        # ã€Œ>ã€ãƒœã‚¿ãƒ³ãŒãªã‘ã‚Œã°(æœ€çµ‚ãƒšãƒ¼ã‚¸ãªã®ã§)çµ‚äº†
        next_page = response.xpath('//nav//li[@class="pagination__arrow-item"]/a[@class="pagination__arrow-item-inner pagination__arrow-item-inner-next"]/@href').extract_first()
        if next_page is None:
            self.logzero_logger.info('ğŸ’» finished. last page = ' + response.request.url)
            return

        self.logzero_logger.info(f'ğŸ›« next url = {next_page}')

        yield scrapy.Request(next_page, callback=self.parse)

    def detail(self, response):
        item = ShopItem()
        self.logzero_logger.info(f'ğŸ’¾ url(detail) = {response.request.url}')
        item['detail_page'] = response.request.url
        item['area_name'] = response.xpath('//ol[@id="gn_info-breadcrumbs-inner"]/li/a[contains(text(), "æ±äº¬")]/../following-sibling::li/a/text()').extract_first()
        for tr in response.xpath('//div[@id="info-table"]/table/tbody'):
            item['shop_name'] = tr.xpath('.//tr/th[contains(text(), "åº—å")]/following-sibling::td/p[@id="info-name"]/text()').get().strip()
            item['tel'] = tr.xpath('.//tr/th[contains(text(), "é›»è©±ç•ªå·ãƒ»FAX")]/following-sibling::td/ul/li/span[@class="number"]/text()').get()

            # data-oã«å…¥ã£ã¦ã‚‹è¬jsonï¼Ÿã‚’parseã—ã¦URLã‚’çµ„ã¿ç«‹ã¦
            data_o = tr.xpath('.//tr/th[contains(text(), "ãŠåº—ã®ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸")]/following-sibling::td/ul/li/a[@class="url go-off"]/@data-o').get()
            if data_o:
                data = json.loads(data_o)
                item['offical_page'] = data['b'] + '://' + data['a']

            zip_code = tr.xpath('.//tr/th[contains(text(), "ä½æ‰€")]/following-sibling::td/p[@class="adr slink"]/text()').get()
            item['zip_code'] = zip_code.strip().replace('ã€’', '') if zip_code else None
            item['address'] = tr.xpath('.//tr/th[contains(text(), "ä½æ‰€")]/following-sibling::td/p[@class="adr slink"]/span[@class="region"]/text()').get().strip()

            text = tr.xpath('.//tr/th[contains(text(), "å–¶æ¥­æ™‚é–“")]/following-sibling::td/div/text()').get()
            item['opening_hours'] = w3lib.html.remove_tags(text).strip() if text else None
            texts = tr.xpath('.//tr/th[contains(text(), "å®šä¼‘æ—¥")]/following-sibling::td/ul/li/text()').getall()
            item['closing_day'] = '\n'.join(texts)

        ## ã‚¸ãƒ£ãƒ³ãƒ«æŠ½å‡º
        # "header-meta-gen-desc"ãŒã‚ã‚Œã°ãã¡ã‚‰ã‚’ã‚¸ãƒ£ãƒ³ãƒ«ã¨ã—ã¦åˆ©ç”¨(è¤‡æ•°ã‚¸ãƒ£ãƒ³ãƒ«ã‚ã‚Š)
        genre_list = []
        for genre in response.xpath('//header[@role="banner"]//dd[@id="header-meta-gen-desc"]/ol/li'):
            genre_list.append(genre.xpath('.//a/text()').get().strip())
        if genre_list:
            item['genre_name'] = '|'.join(genre_list)
        else:
            # "header-meta-gen-desc"ãŒãªã„å ´åˆã¯ä»¥ä¸‹ã‚’åˆ©ç”¨
            # MEMO: ã“ã¡ã‚‰ã‚’åˆ©ç”¨ã™ã‚‹å ´åˆã€ã‚¸ãƒ£ãƒ³ãƒ«åˆ†ã‘ãŒè‡ªç”±å…¥åŠ›ãªã®ã§csv2geojsonã®æ–¹ã§ãã¡ã‚“ã¨ã‚¸ãƒ£ãƒ³ãƒ«ã®åå¯„ã›ã‚’ã‚„ã‚‹å¿…è¦ãŒã‚ã‚‹(ã‚„ã£ãŸ)
            # ãã‚Œã§ã‚‚ã‚¢ãƒ›ã¿ãŸã„ãªã‚¸ãƒ£ãƒ³ãƒ«ãŒå¤šæ•°è¨­å®šã•ã‚Œã¦ã„ã‚‹ãŒã€ãã†ã„ã†ã®ã¯ã—ã‚‡ã†ãŒãªã„â€¦
            item['genre_name'] = response.xpath('//header[@role="banner"]//dd[@id="header-meta-cat-desc"]/text()').get().strip()

        self.logzero_logger.debug(item)
        return item

