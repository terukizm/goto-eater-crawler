import pathlib

import fitz
import pandas as pd
import scrapy
import tabula

from goto_eat_scrapy import settings
from goto_eat_scrapy.items import ShopItem
from goto_eat_scrapy.spiders.abstract import AbstractSpider


class TokyoSpider(AbstractSpider):
    """
    usage:
      $ scrapy crawl tokyo -O output.csv
    """

    name = "tokyo"
    allowed_domains = ["gnavi.co.jp"]
    start_urls = ["https://r.gnavi.co.jp/plan/campaign/gotoeat-tokyo/"]

    CACHE_PATH = pathlib.Path(__file__).parent.parent.parent / ".scrapy" / settings.HTTPCACHE_DIR / name
    CACHE_PATH.mkdir(parents=True, exist_ok=True)

    def parse(self, response):
        for li in response.xpath('//section[@id="c-search__pdf"]/ul/li'):
            pdf_url = li.xpath(".//a/@href").get().strip()
            yield scrapy.Request(pdf_url, callback=self.parse_from_pdf)

    def parse_from_pdf(self, response):
        # MEMO: tempfile, io.stringIOã§ã¯ãã¡ã‚“ã¨å‹•ä½œã—ãªã‹ã£ãŸã®ã§ã€
        # scrapyã®httpcacheã¨åŒã˜å ´æ‰€(settings.HTTPCACHE_DIR)ã«æ›¸ãè¾¼ã‚“ã§ã„ã‚‹
        prefix = (
            response.request.url.replace("https://pr.gnavi.co.jp/promo/gotoeat-tokyo/pdf/", "")
            .replace("/", "-")
            .replace(".pdf", "")
        )
        tmp_pdf = str(self.CACHE_PATH / f"{prefix}.pdf")
        with open(tmp_pdf, "wb") as f:
            f.write(response.body)
            self.logzero_logger.info(f"ğŸ’¾ saved pdf: {response.request.url} > {tmp_pdf}")

        # MEMO: pymupdfã¯æ¯”è¼ƒçš„ç¶ºéº—ã«å–ã‚Œã‚‹ãŒã€ç©ºã‚»ãƒ«ã‚’èª­ã¿é£›ã°ã—ã¦ã—ã¾ã†ãŸã‚ã€ç©ºã‚»ãƒ«ãŒã‚ã‚Šãˆã‚‹URLè¡ŒãŒé›£ã—ã‹ã£ãŸã€‚
        # ã¾ãŸã€Excelã‚·ãƒ¼ãƒˆã®ãƒ˜ãƒƒãƒ€ãƒ¼ã€ãƒ•ãƒƒã‚¿ãƒ¼æ–‡å­—åˆ—(ãƒšãƒ¼ã‚¸ç•ªå·ã¨ã‹ã‚’å«ã‚€ã‚„ã¤)ã¨ã®å…¼ã­åˆã„ãªã®ã‹ã€æœ€çµ‚ãƒšãƒ¼ã‚¸ã ã‘
        # é †ç•ªãŒå…¥ã‚Œæ›¿ã‚ã£ãŸã‚Šã¨ã„ã£ãŸå›ºæœ‰ã®å•é¡Œã‚‚ã‚ã‚Šã€æœ€çµ‚çš„ã«ã¯tabulaã§1ãƒšãƒ¼ã‚¸ãšã¤(è£œæ­£ã—ã¤ã¤)å‡¦ç†ã—ã¦ã„ãæ–¹å¼ã¨ã—ãŸã€‚
        # PDFèª­ã¿è¾¼ã¿ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯è‰²ã€…ã‚ã‚‹ãŒã€èª­ã¿è¾¼ã‚€PDFã«ã‚ˆã£ã¦å‘ãä¸å‘ããŒéå¸¸ã«å¤§ãã„ãŸã‚ã€ä¸€ã¤ãšã¤è©¦ã—ã¦ã„ãã—ã‹ãªã„â€¦

        page_count = fitz.open(tmp_pdf).pageCount
        for page_no in range(1, page_count + 1):
            # tabulaã§1ãƒšãƒ¼ã‚¸å˜ä½ã§CSVã«å¤‰æ›ã—ã¦ã‹ã‚‰dfã«èª­ã¿è¾¼ã‚€
            tmp_csv = self.CACHE_PATH / f"{prefix}_p{page_no}.csv"
            if not tmp_csv.exists():
                tabula.convert_into(tmp_pdf, str(tmp_csv), output_format="csv", pages=page_no, lattice=True)
                self.logzero_logger.info(f"ğŸ’¾ saved csv: >>>>>> {tmp_csv}")

            # ãƒšãƒ¼ã‚¸ã«ã‚ˆã£ã¦ã¯ç©ºè¡Œã€ç©ºåˆ—ã€ä¸è¦ã‚«ãƒ©ãƒ ã‚’å«ã‚€ãŸã‚ã€ãã‚Œã‚‰ã‚’é™¤å»
            df = pd.read_csv(tmp_csv, dtype=str).dropna(how="all").dropna(how="all", axis=1).reset_index(drop=True)
            df.columns = ["ç´™", "é›»å­", "é£²é£Ÿåº—å", "åº—èˆ—ä½æ‰€", "åº—èˆ—é›»è©±ç•ªå·", "URL", "æ¥­æ…‹"]
            df = df.drop(["ç´™", "é›»å­"], axis=1).fillna("")
            for _, row in df.iterrows():
                if row["é£²é£Ÿåº—å"] == "é£²é£Ÿåº—å":
                    # MEMO: ç‰¹å®šãƒšãƒ¼ã‚¸ã§ãƒ˜ãƒƒãƒ€åˆ—ãŒã†ã¾ãå‡¦ç†ã§ããªã„(ãƒ‡ãƒ¼ã‚¿ãƒ¬ã‚³ãƒ¼ãƒ‰ã«å«ã¾ã‚Œã¦ã—ã¾ã†)ã“ã¨ãŒã‚ã‚‹ãŸã‚
                    continue
                item = ShopItem()
                item["shop_name"] = row["é£²é£Ÿåº—å"]
                item["address"] = row["åº—èˆ—ä½æ‰€"]
                item["genre_name"] = row["æ¥­æ…‹"]
                item["tel"] = row["åº—èˆ—é›»è©±ç•ªå·"]
                item["official_page"] = row["URL"]
                yield item
