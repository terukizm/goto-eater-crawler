import re

import scrapy

from goto_eat_scrapy.items import ShopItem
from goto_eat_scrapy.spiders.abstract import AbstractSpider


class KyotoSpider(AbstractSpider):
    """
    usage:
      $ scrapy crawl kyoto -O output.csv
    """

    name = "kyoto"
    allowed_domains = ["kyoto-gotoeat.com"]
    start_urls = ["https://kyoto-gotoeat.com/?s=#keyword"]

    # MEMO: detailã¾ã§å›ã™ã®ã§
    custom_settings = {
        "DOWNLOAD_DELAY": 1.2,
    }

    def parse(self, response):
        self.logzero_logger.info(f"ğŸ’¾ url = {response.request.url}")
        for article in response.xpath('//main[@id="main"]//div[@class="store-item"]'):
            url = response.urljoin(article.xpath('.//a[@class="btnDetail"]/@href').get().strip())
            yield scrapy.Request(response.urljoin(url), callback=self.detail)

        # ã€Œ>ã€ãƒœã‚¿ãƒ³ãŒãªã‘ã‚Œã°(æœ€çµ‚ãƒšãƒ¼ã‚¸ãªã®ã§)çµ‚äº†
        next_page = response.xpath('//div[@role="navigation"]/a[@rel="next"]/@href').extract_first()
        if next_page is None:
            self.logzero_logger.info("ğŸ’» finished. last page = " + response.request.url)
            return

        self.logzero_logger.info(f"ğŸ›« next url = {next_page}")

        yield scrapy.Request(next_page, callback=self.parse)

    def detail(self, response):
        self.logzero_logger.info(f"ğŸ’¾ url(detail) = {response.request.url}")
        article = response.xpath('//main[@id="main"]//div[@class="store-detail"]')

        item = ShopItem()
        item["shop_name"] = article.xpath('.//div[@class="name"]/text()').get().strip()
        item["genre_name"] = (
            article.xpath(
                './/div[@class="store-cont"]/table/tr/th[contains(text(), "ã‚¸ãƒ£ãƒ³ãƒ«")]/following-sibling::td/text()'
            )
            .get()
            .strip()
        )
        item["area_name"] = (
            article.xpath(
                './/div[@class="store-cont"]/table/tr/th[contains(text(), "ã‚¨ãƒªã‚¢")]/following-sibling::td/text()'
            )
            .get()
            .strip()
        )
        item["address"] = (
            article.xpath(
                './/div[@class="store-cont"]/table/tr/th[contains(text(), "ä½æ‰€")]/following-sibling::td/text()'
            )
            .get()
            .strip()
        )
        item["tel"] = (
            article.xpath(
                './/div[@class="store-cont"]/table/tr/th[contains(text(), "é›»è©±ç•ªå·")]/following-sibling::td/text()'
            )
            .get()
        )
        # MEMO: è©³ç´°ãƒšãƒ¼ã‚¸ã«é …ç›®è‡ªä½“ã¯ã‚ã‚‹ãŒã€é›»è©±ç•ªå·ã€å®šä¼‘æ—¥ãŒå…¥ã£ã¦ã‚‹ãƒ‡ãƒ¼ã‚¿ã¯1ä»¶ã‚‚ãªã„(2021/01/18)
        item["opening_hours"] = article.xpath(
            './/div[@class="store-cont"]/table/tr/th[contains(text(), "å–¶æ¥­æ™‚é–“")]/following-sibling::td/text()'
        ).get()
        item["closing_day"] = article.xpath(
            './/div[@class="store-cont"]/table/tr/th[contains(text(), "å®šä¼‘æ—¥")]/following-sibling::td/text()'
        ).get()
        item["official_page"] = article.xpath(
            './/div[@class="store-cont"]/table/tr/th[contains(text(), "U R L")]/following-sibling::td/a/@href'
        ).get()

        gmap_url = article.xpath('.//div[@class="store-cont"]/iframe/@src').get()
        m = re.search("q=(?P<lat>\d+\.\d+)\,(?P<lng>\d+\.\d+)", gmap_url)
        if m:
            item["provided_lat"] = m.group("lat")
            item["provided_lng"] = m.group("lng")

        return item
