
import scrapy
import fitz
import tabula
import pathlib
import pandas as pd
from goto_eat_scrapy import settings
from goto_eat_scrapy.items import ShopItem
from goto_eat_scrapy.spiders.abstract import AbstractSpider

class TokyoPDFSpider(AbstractSpider):
    """
    usage:
      $ scrapy crawl tokyo_pdf -O output.csv
    """
    name = 'tokyo_pdf'
    allowed_domains = [ 'gnavi.co.jp' ]
    start_urls = [ 'https://r.gnavi.co.jp/plan/campaign/gotoeat-tokyo/' ]

    def parse(self, response):
        for li in response.xpath('//section[@id="c-search__pdf"]/ul/li'):
            pdf_url = li.xpath('.//a/@href').get().strip()
            yield scrapy.Request(pdf_url, callback=self.parse_from_pdf)

    def parse_from_pdf(self, response):
        # MEMO: tempfile, io.stringIOã§ã¯ãã¡ã‚“ã¨å‹•ä½œã—ãªã‹ã£ãŸã®ã§ã€
        # scrapyã®httpcacheã¨åŒã˜å ´æ‰€(settings.HTTPCACHE_DIR)ã«æ›¸ãè¾¼ã‚“ã§ã„ã‚‹
        cache_dir = pathlib.Path(__file__).parent.parent.parent / '.scrapy' / settings.HTTPCACHE_DIR / self.name
        prefix = response.request.url.replace('https://pr.gnavi.co.jp/promo/gotoeat-tokyo/pdf/', '').replace('/', '-').replace('.pdf', '')
        tmp_pdf = str(cache_dir / f'{prefix}.pdf')
        with open(tmp_pdf, 'wb') as f:
            f.write(response.body)
            self.logzero_logger.info(f'ğŸ’¾ saved pdf: {response.request.url} > {tmp_pdf}')

        # MEMO: pymupdfã¯æ¯”è¼ƒçš„ç¶ºéº—ã«å–ã‚Œã¦ã„ãŸãŒã€ç©ºã‚»ãƒ«ã‚’èª­ã¿é£›ã°ã—ã¦ã—ã¾ã†ãŸã‚ã€ç©ºã‚»ãƒ«ãŒã‚ã‚Šãˆã‚‹URLã®ã‚ãŸã‚ŠãŒé›£ã—ã‹ã£ãŸã€‚
        # ã¾ãŸã€Excelã‚·ãƒ¼ãƒˆã®ãƒ˜ãƒƒãƒ€ãƒ¼ã€ãƒ•ãƒƒã‚¿ãƒ¼æ–‡å­—åˆ—(ãƒšãƒ¼ã‚¸ç•ªå·ã¨ã‹ã‚’å«ã‚€ã‚„ã¤)ã¨ã®å…¼ã­åˆã„ãªã®ã‹ã€æœ€çµ‚ãƒšãƒ¼ã‚¸ã ã‘
        # é †ç•ªãŒå…¥ã‚Œæ›¿ã‚ã£ãŸã‚Šã¨ã„ã£ãŸå›ºæœ‰ã®å•é¡Œã‚‚ã‚ã‚Šã€tabulaã§1ãƒšãƒ¼ã‚¸ãšã¤(è£œæ­£ã—ã¤ã¤)å‡¦ç†ã—ã¦ã„ãæ–¹å¼ã¨ã—ãŸã€‚
        # PDFèª­ã¿è¾¼ã¿ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯è‰²ã€…ã‚ã‚‹ãŒã€èª­ã¿è¾¼ã‚€PDFã«ã‚ˆã£ã¦å‘ãä¸å‘ããŒéå¸¸ã«å¤§ãã„ãŸã‚ã€ä¸€ã¤ãšã¤è©¦ã—ã¦ã„ãã—ã‹ãªã„â€¦

        page_count = fitz.open(tmp_pdf).pageCount
        for page_no in range(1, page_count+1):
            # tabulaã§1ãƒšãƒ¼ã‚¸å˜ä½ã§CSVã«å¤‰æ›ã—ã¦ã‹ã‚‰dfã«èª­ã¿è¾¼ã‚€
            # MEMO: ãƒšãƒ¼ã‚¸ã«ã‚ˆã£ã¦ã¯ä½™è¨ˆãªç©ºè¡Œã€ç©ºåˆ—ã‚’å«ã‚“ã§ãŠã‚Šã€dfã§æ‰±ã†ã®ã«ä¸ä¾¿ãªãŸã‚
            tmp_csv = cache_dir / f'{prefix}_p{page_no}.csv'
            if not tmp_csv.exists():
                tabula.convert_into(tmp_pdf, str(tmp_csv), output_format="csv", pages=page_no, lattice=True)
                self.logzero_logger.info(f'ğŸ’¾ saved csv: >>>>>> {tmp_csv}')

            # ç©ºè¡Œã€ç©ºåˆ—ã€ä¸è¦ã‚«ãƒ©ãƒ ã‚’é™¤å»
            df = pd.read_csv(tmp_csv, dtype=str).dropna(how='all').dropna(how='all', axis=1).reset_index(drop=True)
            df.columns = ['ç´™', 'é›»å­', 'é£²é£Ÿåº—å', 'åº—èˆ—ä½æ‰€', 'åº—èˆ—é›»è©±ç•ªå·', 'URL', 'æ¥­æ…‹']
            df = df.drop(['ç´™','é›»å­'], axis=1).fillna('')
            for _, row in df.iterrows():
                if row['é£²é£Ÿåº—å'] == 'é£²é£Ÿåº—å':
                    # MEMO: ç‰¹å®šãƒšãƒ¼ã‚¸ã§ãƒ˜ãƒƒãƒ€åˆ—ãŒã†ã¾ãå‡¦ç†ã§ãã¦ã„ãªã„(ãƒ‡ãƒ¼ã‚¿ãƒ¬ã‚³ãƒ¼ãƒ‰ã«å«ã¾ã‚Œã‚‹)ã“ã¨ãŒã‚ã‚‹ãŸã‚
                    continue
                item = ShopItem()
                item['shop_name'] = row['é£²é£Ÿåº—å']
                item['address'] = row['åº—èˆ—ä½æ‰€']
                item['genre_name'] = row['æ¥­æ…‹']
                item['tel'] = row['åº—èˆ—é›»è©±ç•ªå·']
                item['official_page'] = row['URL']
                yield item
