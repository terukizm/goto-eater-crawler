import re
import scrapy
from logzero import logger
from goto_eat_scrapy.items import ShopItem

class TottoriSpider(scrapy.Spider):
    """
    usage:
      $ scrapy crawl tottori -O 31_tottori.csv
    """
    name = 'tottori'
    allowed_domains = [ 'tottori-gotoeat.jp' ]

    start_urls = [
        'https://tottori-gotoeat.jp/store_list/',
    ]

    def parse(self, response):
        # å„åŠ ç›Ÿåº—æƒ…å ±ã‚’æŠ½å‡º
        for article in response.xpath('//div[@class="row"]//div[contains(@class, "store-list_v2")]'):
            item = ShopItem()
            item['shop_name'] = article.xpath('.//div[1]/h2[contains(@class, "mr-3")]/text()').get().strip()

            # MEMO: (2020/11/22) ç†ç”±ã¯ã‚ã‹ã‚‰ãªã„ãŒã€å…¬å¼ã‚µã‚¤ãƒˆã§ã‚‚æ¤œç´¢ã™ã‚‹ã¨ãƒšãƒ¼ã‚¸ã«ã¾ãŸãŒã£ã¦åŒã˜ãƒ‡ãƒ¼ã‚¿ãŒå‡ºã¦ãã‚‹ã‚‚ã®ãŒã‚ã‚‹
            # ä¾‹: ãã°å‡¦äº•ç”°è¾²åœ’(P.5ã€œP.7)
            # ä¾‹: ãƒŸãƒ‹ãƒ¬ã‚¹ãƒˆã‚­ãƒ¥ãƒ¼ãƒ”ãƒƒãƒˆ(P.6ã€œP.7)
            # ã“ã‚Œã‚‰ãŒé‡è¤‡ãƒ¬ã‚³ãƒ¼ãƒ‰ã—ã¦ã‚«ã‚¦ãƒ³ãƒˆã•ã‚Œã¦ã—ã¾ã†ãŒã€å…¬å¼ã‚µã‚¤ãƒˆå´ã§ã‚‚ãã†è¡¨ç¤ºã•ã‚Œã¦ã—ã¾ã†ã®ã§(ç†ç”±ã¯è¬â€¦) å¯¾å‡¦æ–¹æ³•ãŒãªã„ã€‚
            # åº—åã ã‘ã§æ¤œç´¢ã™ã‚‹ã¨é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã¯å‡ºã¦ã“ãªã„ã®ã§ã€ãªã‚“ã‚‰ã‹ã®ã‚¢ãƒ—ãƒªå´ã®ä¸å…·åˆã£ã½ã„æ°—ãŒã™ã‚‹ãŒã€è¬â€¦

            # ä»¥ä¸‹ã¯ä»Šå¾Œå…¥ã‚Œã¦ã‚‚ã‚ˆã„ã‹ã‚‚ã€ã¨ã„ã†é …ç›®
            # area: article.xpath('.//div[1]/p[1]/span[@class="icon-area"]/text()').get().strip()
            # comment: article.xpath('.//div[1]/p[2]/text()').get()

            item['address'] = article.xpath('.//div[2]/p/text()').get().strip()
            item['tel'] = article.xpath('.//div[2]/div[@class="d-flex"]/a/@href').get()

            genres = article.xpath('.//p[@class="mb-0"]/span[contains(@class, "icon-genre")]/text()').getall()
            item['genre_name'] = '|'.join(genres)

            yield item

        # ã€Œ>ã€ãƒœã‚¿ãƒ³ãŒãªã‘ã‚Œã°(æœ€çµ‚ãƒšãƒ¼ã‚¸ãªã®ã§)çµ‚äº†
        next_page = response.xpath('//nav[@role="navigation"]/div[@class="nav-links"]/a[@class="next page-numbers"]/@href').extract_first()
        if next_page is None:
            logger.info('ğŸ’» finished. last page = ' + response.request.url)
            return

        logger.info(f'ğŸ›« next url = {next_page}')

        yield scrapy.Request(next_page, callback=self.parse)
