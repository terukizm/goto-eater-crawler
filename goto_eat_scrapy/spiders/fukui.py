import re
import scrapy
from goto_eat_scrapy.items import ShopItem
from goto_eat_scrapy.spiders.abstract import AbstractSpider

class FukuiSpider(AbstractSpider):
    """
    usage:
      $ scrapy crawl fukui -O fukui.csv
    """
    name = 'fukui'
    allowed_domains = [ 'gotoeat-fukui.com' ] # .comã¨ã¯

    def start_requests(self):
        params = {'Keyword': '', 'Action': 'text_search'}
        yield scrapy.FormRequest('https://gotoeat-fukui.com/shop/search.php', \
            callback=self.search, method='POST', \
            formdata=params)

    def search(self, response):
        # ç¦äº•çœŒã¯æ¤œç´¢çµæœã®ãƒšãƒ¼ã‚¸ãƒ³ã‚°ãªã—
        self.logzero_logger.info(f'ğŸ’¾ url(search) = {response.request.url}')
        for article in response.xpath('//div[@class="result"]/ul/li'):
            url = article.xpath('.//a/@href').get().strip()
            yield scrapy.Request(response.urljoin(url), callback=self.detail)

    def detail(self, response):
        self.logzero_logger.info(f'ğŸ’¾ url(detail) = {response.request.url}')
        item = ShopItem()
        item['shop_name'] = response.xpath('//div[@id="contents"]/h3/text()').get().strip()
        item['area_name'] = response.xpath('//div[@id="contents"]/div[@class="icon"]/span[@class="area"]/text()').get().strip()
        item['detail_page'] = response.request.url

        for dl in response.xpath('//div[@id="contents"]/dl'):
            # MEMO: ã‚¸ãƒ£ãƒ³ãƒ«æŒ‡å®šãŒã•ã‚Œã¦ã„ãªã„(dt[1]ãŒç©ºã®)ãƒ‡ãƒ¼ã‚¿ã€ã€Œã‚°ãƒ«ãƒ¡æ°‘å®¿ ã¯ã¾ã‚‚ã¨ã€ãŒã‚ã‚Šã€ãã®å ´åˆfollowing-siblingãŒå¤‰ãªã¨ã“ã‚
            # (dd/text()ã®å€¤ãŒå­˜åœ¨ã™ã‚‹ã€Œä½æ‰€ã€ï¼Ÿ)ã‚’è¦‹ã«è¡Œã£ã¦ã—ã¾ã†ã®ã§ã€æš«å®šçš„ã«ã‚¸ãƒ£ãƒ³ãƒ«éƒ¨åˆ†ã ã‘ç›´æŒ‡å®šã§å®Ÿè£…
            # å‚è€ƒ: https://gotoeat-fukui.com/shop/?id=180097  (ã«ã—ã¦ã‚‚ç”»åƒãŒã†ã¾ãã†ã§ã¤ã‚‰ã„)

            genre_name = dl.xpath('.//dt[1]/dd/text()').get()
            genre_name = genre_name.strip() if genre_name else ''   # ã¯ã¾ã‚‚ã¨ã ã‘ã®ä¾‹å¤–å¯¾å¿œ
            item['genre_name'] = genre_name.replace('ã€', '|')  # è¤‡æ•°ã‚¸ãƒ£ãƒ³ãƒ«æŒ‡å®šã‚ã‚Š
            # å…ƒãƒ‡ãƒ¼ã‚¿ãŒä¿®æ­£ã•ã‚Œã‚Œã°ä»¥ä¸‹ã®å®Ÿè£…ã§ã‚ˆã„
            # genre_name = dl.xpath('.//dt[contains(text(), "ã‚¸ãƒ£ãƒ³ãƒ«")]/following-sibling::dd/text()').get().strip()

            item['tel'] = dl.xpath('.//dt[contains(text(), "é›»ã€€ã€€è©±")]/following-sibling::dd/a/text()').get().strip()
            item['address'] = dl.xpath('.//dt[contains(text(), "ä½ã€€ã€€æ‰€")]/following-sibling::dd/text()').get().strip()
            item['opening_hours'] = dl.xpath('.//dt[contains(text(), "å–¶æ¥­æ™‚é–“")]/following-sibling::dd/text()').get().strip()
            item['closing_day'] = dl.xpath('.//dt[contains(text(), "å®š ä¼‘ æ—¥")]/following-sibling::dd/text()').get().strip()
            item['offical_page'] = dl.xpath('.//dt[contains(text(), "HPãƒ»SNS")]/following-sibling::dd/text()').get()

        self.logzero_logger.debug(item)
        return item
