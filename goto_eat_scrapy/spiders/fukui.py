import re
import scrapy
from goto_eat_scrapy.items import ShopItem
from goto_eat_scrapy.spiders.abstract import AbstractSpider

class FukuiSpider(AbstractSpider):
    """
    usage:
      $ scrapy crawl fukui -O fukui.csv
    """
    name = 'fukui'
    allowed_domains = [ 'gotoeat-fukui.com' ] # .comã¨ã¯

    def start_requests(self):
        params = {'Keyword': '', 'Action': 'text_search'}
        yield scrapy.FormRequest('https://gotoeat-fukui.com/shop/search.php', \
            callback=self.search, method='POST', \
            formdata=params)

    def search(self, response):
        # ç¦äº•çœŒã¯æ¤œç´¢çµæœã®ãƒšãƒ¼ã‚¸ãƒ³ã‚°ãªã—
        self.logzero_logger.info(f'ğŸ’¾ url(search) = {response.request.url}')
        for article in response.xpath('//div[@class="result"]/ul/li'):
            url = article.xpath('.//a/@href').get().strip()
            yield scrapy.Request(response.urljoin(url), callback=self.detail)

    def detail(self, response):
        self.logzero_logger.info(f'ğŸ’¾ url(detail) = {response.request.url}')
        item = ShopItem()
        item['shop_name'] = response.xpath('//div[@id="contents"]/h3/text()').get().strip()
        # TODO: ç¦äº•ã«é™ã‚‰ãšã€csvã«detailã®urlã€å…¥ã‚Œã¦ã‚„ã‚‹ã»ã†ãŒã„ã„ã‹ã‚‚ã—ã‚Œãªã„
        # FIXME: ã‚¸ãƒ£ãƒ³ãƒ«æŒ‡å®šãŒã•ã‚Œã¦ã„ãªã„(ddãŒç©ºã®)ã€Œã‚°ãƒ«ãƒ¡æ°‘å®¿ ã¯ã¾ã‚‚ã¨ã€ãŒã‚ã‚Šã€ãã®å ´åˆã«following-siblingãŒå¤‰ãªã¨ã“ã‚
        # (dd/text()ã®å€¤ãŒå­˜åœ¨ã™ã‚‹ã€Œä½æ‰€ã€ï¼Ÿ)ã‚’è¦‹ã«è¡Œã£ã¦ã—ã¾ã†
        # å‚è€ƒ: https://gotoeat-fukui.com/shop/?id=180097  (ã«ã—ã¦ã‚‚ç”»åƒãŒã†ã¾ãã†ã§ã¤ã‚‰ã„)
        for dl in response.xpath('//div[@id="contents"]/dl'):
            # è¤‡æ•°ã‚¸ãƒ£ãƒ³ãƒ«æŒ‡å®šã‚ã‚Š
            genre_name = dl.xpath('.//dt[contains(text(), "ã‚¸ãƒ£ãƒ³ãƒ«")]/following-sibling::dd/text()').get().strip()
            item['genre_name'] = genre_name.replace('ã€', '|')
            item['tel'] = dl.xpath('.//dt[contains(text(), "é›»ã€€ã€€è©±")]/following-sibling::dd/a/text()').get().strip()
            item['address'] = dl.xpath('.//dt[contains(text(), "ä½ã€€ã€€æ‰€")]/following-sibling::dd/text()').get().strip()
            item['opening_hours'] = dl.xpath('.//dt[contains(text(), "å–¶æ¥­æ™‚é–“")]/following-sibling::dd/text()').get().strip()
            item['closing_day'] = dl.xpath('.//dt[contains(text(), "å®š ä¼‘ æ—¥")]/following-sibling::dd/text()').get().strip()
            item['offical_page'] = dl.xpath('.//dt[contains(text(), "HPãƒ»SNS")]/following-sibling::dd/text()').get()

        self.logzero_logger.debug(item)
        return item
