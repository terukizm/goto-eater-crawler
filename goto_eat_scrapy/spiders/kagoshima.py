
import scrapy
import tabula
import pathlib
import pandas as pd
from goto_eat_scrapy import settings
from goto_eat_scrapy.items import ShopItem
from goto_eat_scrapy.spiders.abstract import AbstractSpider

class KagoshimaSpider(AbstractSpider):
    """
    usage:
      $ scrapy crawl kagoshima -O kagoshima.csv
    """
    name = 'kagoshima'
    allowed_domains = [ 'www.kagoshima-cci.or.jp' ]
    start_urls = [
        'http://www.kagoshima-cci.or.jp/wp-content/uploads/2020/11/0.pdf',     # ã€Œé¹¿å…å³¶å¸‚å…¨åŸŸã€ 25ãƒšãƒ¼ã‚¸ãã‚‰ã„
        'http://www.kagoshima-cci.or.jp/wp-content/uploads/2020/11/10.pdf',    # ã€Œè–©æ‘©å·å†…å¸‚ã€  3ãƒšãƒ¼ã‚¸ãã‚‰ã„
        'http://www.kagoshima-cci.or.jp/wp-content/uploads/2020/11/11.pdf',    # ã€Œé¹¿å±‹å¸‚ã€ã€€3ãƒšãƒ¼ã‚¸ãã‚‰ã„
        'http://www.kagoshima-cci.or.jp/wp-content/uploads/2020/11/12.pdf',    # ã€Œæ•å´å¸‚ã€ã€€1ãƒšãƒ¼ã‚¸ãã‚‰ã„
        'http://www.kagoshima-cci.or.jp/wp-content/uploads/2020/11/13.pdf',    # ã€Œé˜¿ä¹…æ ¹å¸‚ã€ã€€1ãƒšãƒ¼ã‚¸ãã‚‰ã„
        'http://www.kagoshima-cci.or.jp/wp-content/uploads/2020/11/14.pdf',    # ã€Œå¥„ç¾å¸‚ã€ã€€3ãƒšãƒ¼ã‚¸ãã‚‰ã„
        'http://www.kagoshima-cci.or.jp/wp-content/uploads/2020/11/15.pdf',    # ã€Œå—ã•ã¤ã¾å¸‚ã€ã€€2ãƒšãƒ¼ã‚¸ãã‚‰ã„
        'http://www.kagoshima-cci.or.jp/wp-content/uploads/2020/11/16.pdf',    # ã€Œå‡ºæ°´å¸‚ã€ã€€2ãƒšãƒ¼ã‚¸ãã‚‰ã„
        'http://www.kagoshima-cci.or.jp/wp-content/uploads/2020/11/17.pdf',    # ã€ŒæŒ‡å®¿å¸‚ã€ã€€2ãƒšãƒ¼ã‚¸ãã‚‰ã„
        'http://www.kagoshima-cci.or.jp/wp-content/uploads/2020/11/18.pdf',    # ã€Œã„ã¡ãä¸²æœ¨é‡å¸‚ã€ã€€1ãƒšãƒ¼ã‚¸ãã‚‰ã„
        'http://www.kagoshima-cci.or.jp/wp-content/uploads/2020/11/19.pdf',    # ã€Œéœ§å³¶å¸‚ã€ã€€3ãƒšãƒ¼ã‚¸ãã‚‰ã„
        'http://www.kagoshima-cci.or.jp/wp-content/uploads/2020/11/20.pdf',    # ã€Œå§¶è‰¯å¸‚ã€ã€€2ãƒšãƒ¼ã‚¸ãã‚‰ã„
        'http://www.kagoshima-cci.or.jp/wp-content/uploads/2020/11/21.pdf',    # ã€Œãã®ä»–åœ°åŸŸã€ã€€3ãƒšãƒ¼ã‚¸ãã‚‰ã„
    ]

    def __init__(self, logfile=None, *args, **kwargs):
        super().__init__(logfile, *args, **kwargs)

    def parse(self, response):
        # TODO: å®Ÿéš›ã«PDFãƒ‡ãƒ¼ã‚¿ã¨è¡Œæ•°ã‚’çªãåˆã‚ã›ã¦ã®çµæœç¢ºèª

        # MEMO: tempfile, io.stringIOã§ã¯tabula-pyãŒãã¡ã‚“ã¨å‹•ä½œã—ãªã‹ã£ãŸã®ã§ã€
        # scrapyã®httpcacheã¨åŒã˜å ´æ‰€(settings.HTTPCACHE_DIR)ã«æ›¸ãè¾¼ã‚“ã§ã„ã‚‹
        cache_dir = pathlib.Path(__file__).parent.parent.parent / '.scrapy' / settings.HTTPCACHE_DIR / self.name
        prefix = response.request.url.replace('http://www.kagoshima-cci.or.jp/wp-content/uploads/', '').replace('/', '-').replace('.pdf', '')
        tmp_pdf = str(cache_dir / f'{prefix}.pdf')
        tmp_csv = str(cache_dir / f'{prefix}.csv')
        with open(tmp_pdf, 'wb') as f:
            f.write(response.body)
        self.logzero_logger.info(f'ğŸ’¾ saved pdf: {response.request.url} > {tmp_pdf}')

        tabula.convert_into(tmp_pdf, tmp_csv, output_format="csv", pages='all', lattice=True)
        self.logzero_logger.info(f'ğŸ’¾ converted csv: {tmp_pdf} > {tmp_csv}')

        df = pd.read_csv(tmp_csv, header=1, names=('åº—èˆ—å', 'æ‰€åœ¨åœ°'))
        df = df.dropna(subset=['åº—èˆ—å'])[df['åº—èˆ—å'] != 'åº—èˆ—å'] # ãƒ˜ãƒƒãƒ€è¡Œã«ç›¸å½“ã™ã‚‹éƒ¨åˆ†ã‚’é™¤å»
        for _, row in df.iterrows():
            item = ShopItem()
            item['shop_name'] = row['åº—èˆ—å'].strip()
            item['address'] = row['æ‰€åœ¨åœ°'].strip()
            item['genre_name'] = None   # é¹¿å…å³¶ã®PDFã¯ã‚¸ãƒ£ãƒ³ãƒ«æƒ…å ±ãªã—
            self.logzero_logger.debug(item)
            yield item
