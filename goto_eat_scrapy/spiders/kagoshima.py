import scrapy

from goto_eat_scrapy.items import ShopItem
from goto_eat_scrapy.spiders.abstract import AbstractSpider


class KagoshimaSpider(AbstractSpider):
    """
    usage:
      $ scrapy crawl kagoshima -O kagoshima.csv
    """

    name = "kagoshima"
    allowed_domains = ["kagoshima-cci.or.jp"]
    start_urls = ["http://www.kagoshima-cci.or.jp/?p=20375"]

    # MEMO: é¹¿å…å³¶ã¯2020/12/04ï¼Ÿã«pdfã‚’ã‚„ã‚ã¦htmlã«ã—ã¦ãã‚ŒãŸãŒã€HTMLã‚½ãƒ¼ã‚¹ã‚’è¦‹ã‚‹ã¨é‡è¤‡ã—ãŸå†…å®¹ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã€‚(display:noneã§å‡ºã—åˆ†ã‘ã—ã¦ã‚‹ã ã‘)
    # ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã®å†…å®¹ã¨ä»¶æ•°ã‹ã‚‰è¦‹ã¦ã€ãŠãã‚‰ãã€Œé¹¿å…å³¶å¸‚å…¨åŸŸã€ã¨ã€Œãã®ä»–åœ°åŸŸ(é¹¿å±‹å¸‚ã€œãã®ä»–åœ°åŸŸ)ã€ã§ï¼’ã¤ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã€
    # ãã‚Œã‚’(Excelã®è¡Œã®éè¡¨ç¤ºã¨ã‹ã§)å‡ºã—åˆ†ã‘ã—ã€Excelã®Webãƒšãƒ¼ã‚¸ç™ºè¡Œã‚¦ã‚£ã‚¶ãƒ¼ãƒ‰ã§å‡ºåŠ›ã—ã¦ã„ã‚‹ã¨æ€ã‚ã‚Œã‚‹ã€‚
    # é¹¿å…å³¶ã¯ã‚‚ã†3å›ãã‚‰ã„å‡ºåŠ›å½¢å¼ãŒå¤‰ã‚ã£ã¦ã„ã‚‹ã®ã§ã€ã‚‚ã†è«¦ã‚ã¦ã‚„ã£ã¤ã‘ä»•äº‹ã§ã„ã...
    area_list = [
        "é¹¿å…å³¶å¸‚å…¨åŸŸ",
        # 'ã€‡è–©æ‘©å·å†…å¸‚',
        # 'ã€‡é¹¿å±‹å¸‚',
        # 'ã€‡æ•å´å¸‚',
        # 'ã€‡é˜¿ä¹…æ ¹å¸‚',
        # 'ã€‡å¥„ç¾å¸‚',
        # 'ã€‡å—ã•ã¤ã¾å¸‚',
        # 'ã€‡å‡ºæ°´å¸‚',
        # 'ã€‡æŒ‡å®¿å¸‚',
        # 'ã€‡ã„ã¡ãä¸²æœ¨é‡å¸‚',
        # 'ã€‡éœ§å³¶å¸‚',
        # 'ã€‡å§¶è‰¯å¸‚',
        "ã€‡ãã®ä»–åœ°åŸŸ",
    ]
    not_target_area_list = [
        "å¤©æ–‡é¤¨åœ°åŒº",
        "é¹¿å…å³¶ä¸­å¤®é§…åœ°åŒº",
        "ä¸­å¤®åœ°åŒº",
        "ä¸Šç”ºåœ°åŒº",
        "é´¨æ± åœ°åŒº",
        "åŸè¥¿åœ°åŒº",
        "æ­¦ãƒ»ç”°ä¸Šåœ°åŒº",
        "è°·å±±åŒ—éƒ¨åœ°åŒº",
        "è°·å±±åœ°åŒº",
        "ä¼Šæ•·ãƒ»å‰é‡åœ°åŒº",
        "æ¡œå³¶ãƒ»å‰ç”°ãƒ»å–œå…¥ãƒ»æ¾å…ƒãƒ»éƒ¡å±±åœ°åŒº",
        "â—‡é£Ÿäº‹åˆ¸è³¼å…¥æƒ…å ±ã¯ã“ã¡ã‚‰",
    ]

    def parse(self, response):
        for p in response.xpath('//div[@id="contents_layer"]/span/p'):
            area_name = p.xpath(".//a/text()").get()
            self.logzero_logger.info(f"ğŸ—» area_name = {area_name}")
            if not area_name:
                continue
            if area_name in self.not_target_area_list:
                continue
            if area_name in self.area_list:
                url = p.xpath(".//a/@href").get().strip()
                yield scrapy.Request(url, callback=self.parse_from_area_html, meta={"area_name": area_name})
            else:
                pass

    def parse_from_area_html(self, response):
        area_name = response.meta["area_name"]
        for article in response.xpath("//table/tr"):
            if article.xpath('.//td[2]//*[contains(text(), "æ¤œç´¢")]').get():
                item = ShopItem()
                # MEMO: åº—èˆ—åã€ä½æ‰€ã«æ”¹è¡ŒãŒå…¥ã£ã¦ã‚‹ã‚‚ã®ãŒã‚ã‚‹(item pipelineã§å¯¾å¿œ)
                item["shop_name"] = article.xpath(".//td[3]/text()").get().strip()
                address = article.xpath("./td[4]/text()").get().strip()
                item["address"] = f"é¹¿å…å³¶å¸‚{address}" if area_name == "é¹¿å…å³¶å¸‚å…¨åŸŸ" else address
                # item['genre_name'] = None   # é¹¿å…å³¶ã¯ã‚¸ãƒ£ãƒ³ãƒ«æƒ…å ±ãªã—

                # MEMO: ã‚¨ãƒªã‚¢åã¯htmlã‹ã‚‰å–ã‚Œãªãã‚‚ãªã„ãŒã€Excelãƒ™ãƒ¼ã‚¹ã®è¡¨æ§‹é€ ã«ãªã£ã¦ã„ã‚‹ã®ã§ç›¸å½“ã—ã‚“ã©ã„
                # æœ€å¾Œã¾ã§æœ¬æ°—ã§ã“ã®Webãƒšãƒ¼ã‚¸å½¢å¼ã§è¡Œãæ„Ÿã˜ã§ã€ã‚¨ãƒªã‚¢åã®éœ€è¦ãŒã‚ã¡ã‚ƒãã¡ã‚ƒé«˜ã‘ã‚Œã°ã€è«¦ã‚ã¦å¯¾å¿œã™ã‚‹â€¦ã‹ã‚‚

                yield item
