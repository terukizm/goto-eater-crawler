
import scrapy
import fitz
import pathlib
import pandas as pd
from goto_eat_scrapy import settings
from goto_eat_scrapy.items import ShopItem
from goto_eat_scrapy.spiders.abstract import AbstractSpider

class KagoshimaSpider(AbstractSpider):
    """
    usage:
      $ scrapy crawl kagoshima -O kagoshima.csv
    """
    name = 'kagoshima'
    allowed_domains = [ 'kagoshima-cci.or.jp' ]
    start_urls = ['http://www.kagoshima-cci.or.jp/?p=20375']

    # FIXME: ã‚„ã£ã¤ã‘ãŒã™ãã‚‹...
    area_list = [
        'é¹¿å…å³¶å¸‚å…¨åŸŸ',
        'ã€‡è–©æ‘©å·å†…å¸‚',
        'ã€‡é¹¿å±‹å¸‚',
        'ã€‡æ•å´å¸‚',
        'ã€‡é˜¿ä¹…æ ¹å¸‚',
        'ã€‡å¥„ç¾å¸‚',
        'ã€‡å—ã•ã¤ã¾å¸‚',
        'ã€‡å‡ºæ°´å¸‚',
        'ã€‡æŒ‡å®¿å¸‚',
        'ã€‡ã„ã¡ãä¸²æœ¨é‡å¸‚',
        'ã€‡éœ§å³¶å¸‚',
        'ã€‡å§¶è‰¯å¸‚',
        'ã€‡ãã®ä»–åœ°åŸŸ',
    ]
    not_target_area_list = [
        'å¤©æ–‡é¤¨åœ°åŒº',
        'é¹¿å…å³¶ä¸­å¤®é§…åœ°åŒº',
        'ä¸­å¤®åœ°åŒº',
        'ä¸Šç”ºåœ°åŒº',
        'é´¨æ± åœ°åŒº',
        'åŸè¥¿åœ°åŒº',
        'æ­¦ãƒ»ç”°ä¸Šåœ°åŒº',
        'è°·å±±åŒ—éƒ¨åœ°åŒº',
        'è°·å±±åœ°åŒº',
        'ä¼Šæ•·ãƒ»å‰é‡åœ°åŒº',
        'æ¡œå³¶ãƒ»å‰ç”°ãƒ»å–œå…¥ãƒ»æ¾å…ƒãƒ»éƒ¡å±±åœ°åŒº',
        'â—‡é£Ÿäº‹åˆ¸è³¼å…¥æƒ…å ±ã¯ã“ã¡ã‚‰',
    ]

    def parse(self, response):
        for p in response.xpath('//div[@id="contents_layer"]/span/p'):
            text = p.xpath('.//a/text()').get()
            if not text:
                continue
            if text in self.not_target_area_list:
                continue
            if text in self.area_list:
                pdf_url = p.xpath('.//a/@href').get().strip()
                yield scrapy.Request(pdf_url, callback=self.parse_from_pdf)
            else:
                # ãŸã®ã‚€ãkcci...
                self.logzero_logger.warn(f'é¹¿å…å³¶å•†å·¥ä¼šè­°æ‰€ã‚¨ãƒ©ãƒ¼: ã€Œ{text}ã€ is not found.')


    def parse_from_pdf(self, response):
        # MEMO: tempfile, io.stringIOã§ã¯tabula-pyãŒãã¡ã‚“ã¨å‹•ä½œã—ãªã‹ã£ãŸã®ã§ã€
        # scrapyã®httpcacheã¨åŒã˜å ´æ‰€(settings.HTTPCACHE_DIR)ã«æ›¸ãè¾¼ã‚“ã§ã„ã‚‹
        cache_dir = pathlib.Path(__file__).parent.parent.parent / '.scrapy' / settings.HTTPCACHE_DIR / self.name
        prefix = response.request.url.replace('http://www.kagoshima-cci.or.jp/wp-content/uploads/', '').replace('/', '-').replace('.pdf', '')
        tmp_pdf = str(cache_dir / f'{prefix}.pdf')
        with open(tmp_pdf, 'wb') as f:
            f.write(response.body)
        self.logzero_logger.info(f'ğŸ’¾ saved pdf: {response.request.url} > {tmp_pdf}')

        # tabula-py, Camelot, pdfminer, pdfboxã¨è©¦ã—ã€æœ€çµ‚çš„ã«pymupdfã‚’åˆ©ç”¨
        # PDFãŒã€Œç½«ç·šãªã—ã€ã€Œãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãŒä¸è¦å‰‡(é ­æ–‡å­—ãŒã‚ã‚‹ãŸã‚)ã€ã¨ã„ã†ã“ã¨ã§å¤§å¤‰å‡¦ç†ãŒã—ã«ãã„â€¦
        # ã•ã‚‰ã«ã€Œé¹¿å…å³¶å¸‚å…¨åŸŸã€ã®PDFã®å ´åˆã€é¹¿å…å³¶å¸‚ãŒçœç•¥ã•ã‚Œã¦ã„ã‚‹
        for page in fitz.open(tmp_pdf):
            lines = page.getText("text").split('\n')
            for i, row in enumerate(lines):
                if row.startswith('æ¤œç´¢'):
                    item = ShopItem()
                    item['shop_name'] = row.replace('æ¤œç´¢ ', '')
                    item['address'] = 'é¹¿å…å³¶å¸‚{}'.format(lines[i+1]) if tmp_pdf.endswith('0.pdf') else lines[i+1]
                    item['genre_name'] = None   # é¹¿å…å³¶ã®PDFã¯ã‚¸ãƒ£ãƒ³ãƒ«æƒ…å ±ãªã—
                    self.logzero_logger.debug(item)
                    yield item

        # MEMO: é›‘ãªPDFã¨è¡Œæ•°ã‚’çªãåˆã‚ã›ã¦ã®çµæœç¢ºèª
        #
        # ãƒ–ãƒ©ã‚¦ã‚¶ã§ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ã«å…¨é¸æŠã—ãŸã‚ã¨ã«
        # $ pbpaste | grep æ¤œç´¢ | wc -l
        # 1466

        ### 2020/11/30æ™‚ç‚¹ã§
        # 1466 + 146 + 146 + 34 + 42 + 139 + 65 + 101 + 104 + 50 + 174 + 74 + 145
        # = 2686
