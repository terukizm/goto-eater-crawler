import re
import scrapy
from goto_eat_scrapy.items import ShopItem
from goto_eat_scrapy.spiders.abstract import AbstractSpider


class KagawaSpider(AbstractSpider):
    """
    usage:
      $ scrapy crawl kagawa -O kagawa.csv
    """

    name = "kagawa"
    allowed_domains = ["kagawa-gotoeat.com"]
    start_urls = ["https://www.kagawa-gotoeat.com/gtes/store-list?fstr=&mode=only"]

    def parse(self, response):
        self.logzero_logger.info(f"ğŸ’¾ url = {response.request.url}")
        for article in response.xpath('//div[@class="container"]/div[contains(@class, "store-list")]'):
            item = ShopItem()
            item["shop_name"] = article.xpath(".//h4/text()").get().strip()
            item["area_name"] = (
                article.xpath('.//table/tr/th/span[contains(text(), "ã‚¨ãƒªã‚¢")]/../following-sibling::td/text()')
                .get()
                .strip()
            )

            # è¤‡æ•°ã‚¸ãƒ£ãƒ³ãƒ«ã®å ´åˆãŒã‚ã‚Šã€åŒºåˆ‡ã‚Šæ–‡å­—ãŒ "ï½œ"(å…¨è§’ãƒ‘ã‚¤ãƒ—) ã§æŒ‡å®šã•ã‚Œã¦ã„ã‚‹
            # CSVã®è¤‡æ•°ã‚¸ãƒ£ãƒ³ãƒ«æŒ‡å®šæ™‚ã®å†…éƒ¨è¡¨ç¾ã«åˆã‚ã›ã¦ "|"(åŠè§’ãƒ‘ã‚¤ãƒ—) ã«ç½®æ›
            genre = article.xpath(
                './/table/tr/th/span[contains(text(), "æ–™ç†ã‚¸ãƒ£ãƒ³ãƒ«")]/../following-sibling::td/text()'
            ).get()
            item["genre_name"] = genre.strip().replace("ï½œ", "|")

            item["tel"] = (
                article.xpath('.//table/tr/th/span[contains(text(), "é›»è©±ç•ªå·")]/../following-sibling::td/text()')
                .get()
                .strip()
            )
            item["address"] = (
                article.xpath('.//table/tr/th/span[contains(text(), "ä½æ‰€")]/../following-sibling::td/text()')
                .get()
                .strip()
            )

            yield item

        # ã€Œ>ã€ãƒœã‚¿ãƒ³ãŒãªã‘ã‚Œã°(æœ€çµ‚ãƒšãƒ¼ã‚¸ãªã®ã§)çµ‚äº†
        next_page = response.xpath('//div[@role="navigation"]/a[@rel="next"]/@href').extract_first()
        if next_page is None:
            self.logzero_logger.info("ğŸ’» finished. last page = " + response.request.url)
            return

        next_page = response.urljoin(next_page)
        self.logzero_logger.info(f"ğŸ›« next url = {next_page}")

        yield scrapy.Request(next_page, callback=self.parse)
