import os
import urllib.parse
import re
import scrapy
from goto_eat_scrapy.items import ShopItem
from goto_eat_scrapy.spiders.abstract import AbstractSpider


class SaitamaSpider(AbstractSpider):
    """
    usage:
      $ scrapy crawl saitama -O saitama.csv
    """

    name = "saitama"
    allowed_domains = ["saitama-goto-eat.com"]

    area_list = [
        "ã•ã„ãŸã¾å¸‚è¥¿åŒº",
        "ã•ã„ãŸã¾å¸‚åŒ—åŒº",
        "ã•ã„ãŸã¾å¸‚å¤§å®®åŒº",
        "ã•ã„ãŸã¾å¸‚è¦‹æ²¼åŒº",
        "ã•ã„ãŸã¾å¸‚ä¸­å¤®åŒº",
        "ã•ã„ãŸã¾å¸‚æ¡œåŒº",
        "ã•ã„ãŸã¾å¸‚æµ¦å’ŒåŒº",
        "ã•ã„ãŸã¾å¸‚å—åŒº",
        "ã•ã„ãŸã¾å¸‚ç·‘åŒº",
        "ã•ã„ãŸã¾å¸‚å²©æ§»åŒº",
        "å·è¶Šå¸‚",
        "ç†Šè°·å¸‚",
        "å·å£å¸‚",
        "è¡Œç”°å¸‚",
        "ç§©çˆ¶å¸‚",
        "æ‰€æ²¢å¸‚",
        "é£¯èƒ½å¸‚",
        "åŠ é ˆå¸‚",
        "æœ¬åº„å¸‚",
        "æ±æ¾å±±å¸‚",
        "æ˜¥æ—¥éƒ¨å¸‚",
        "ç‹­å±±å¸‚",
        "ç¾½ç”Ÿå¸‚",
        "é´»å·£å¸‚",
        "æ·±è°·å¸‚",
        "ä¸Šå°¾å¸‚",
        "è‰åŠ å¸‚",
        "è¶Šè°·å¸‚",
        "è•¨å¸‚",
        "æˆ¸ç”°å¸‚",
        "å…¥é–“å¸‚",
        "æœéœå¸‚",
        "å¿—æœ¨å¸‚",
        "å’Œå…‰å¸‚",
        "æ–°åº§å¸‚",
        "æ¡¶å·å¸‚",
        "ä¹…å–œå¸‚",
        "åŒ—æœ¬å¸‚",
        "å…«æ½®å¸‚",
        "å¯Œå£«è¦‹å¸‚",
        "ä¸‰éƒ·å¸‚",
        "è“®ç”°å¸‚",
        "å‚æˆ¸å¸‚",
        "å¹¸æ‰‹å¸‚",
        "é¶´ãƒ¶å³¶å¸‚",
        "æ—¥é«˜å¸‚",
        "å‰å·å¸‚",
        "ãµã˜ã¿é‡å¸‚",
        "ç™½å²¡å¸‚",
        "åŒ—è¶³ç«‹éƒ¡ä¼Šå¥ˆç”º",
        "å…¥é–“éƒ¡ä¸‰èŠ³ç”º",
        "å…¥é–“éƒ¡æ¯›å‘‚å±±ç”º",
        "å…¥é–“éƒ¡è¶Šç”Ÿç”º",
        "æ¯”ä¼éƒ¡æ»‘å·ç”º",
        "æ¯”ä¼éƒ¡åµå±±ç”º",
        "æ¯”ä¼éƒ¡å°å·ç”º",
        "æ¯”ä¼éƒ¡å·å³¶ç”º",
        "æ¯”ä¼éƒ¡å‰è¦‹ç”º",
        "æ¯”ä¼éƒ¡é³©å±±ç”º",
        "æ¯”ä¼éƒ¡ã¨ããŒã‚ç”º",
        "ç§©çˆ¶éƒ¡æ¨ªç€¬ç”º",
        "ç§©çˆ¶éƒ¡çš†é‡ç”º",
        "ç§©çˆ¶éƒ¡é•·ç€ç”º",
        "ç§©çˆ¶éƒ¡å°é¹¿é‡ç”º",
        "ç§©çˆ¶éƒ¡æ±ç§©çˆ¶æ‘",
        "å…ç‰éƒ¡ç¾é‡Œç”º",
        "å…ç‰éƒ¡ç¥å·ç”º",
        "å…ç‰éƒ¡ä¸Šé‡Œç”º",
        "å¤§é‡Œéƒ¡å¯„å±…ç”º",
        "å—åŸ¼ç‰éƒ¡å®®ä»£ç”º",
        "åŒ—è‘›é£¾éƒ¡æ‰æˆ¸ç”º",
        "åŒ—è‘›é£¾éƒ¡æ¾ä¼ç”º",
        "åŒ—åŸ¼ç‰éƒ¡é¨è¥¿ç”º",
    ]

    def start_requests(self):
        # åŸ¼ç‰çœŒã¯å„å¸‚åŒºç”ºæ‘ã®å›ºå®šhtmlã‚’jQueryã§èª­ã‚“ã§ã‚‹ã ã‘
        # @see https://saitama-goto-eat.com/store.html
        for area in self.area_list:
            yield scrapy.Request(
                url=f"https://saitama-goto-eat.com/store/{area}.html", callback=self.parse, meta={"area_name": area}
            )

    def parse(self, response):
        self.logzero_logger.info(f"ğŸ’¾ url = {response.request.url}")
        area_name = response.meta["area_name"]
        for genre in response.xpath('//div[@class="tab_content"]'):
            genre_name = genre.xpath('.//div[@class="aria_genre"]/text()').get().strip()
            for article in genre.xpath('.//div[@class="aria_store_content"]/div[@class="storebox"]'):
                item = ShopItem(
                    area_name=area_name,
                    genre_name=genre_name,
                    shop_name=article.xpath(".//span[1]/text()").get().strip(),
                    # MEMO: span[2]ã¯ã²ã¨ã¨ãŠã‚Šè¦‹ãŸãŒä¸€ã¤ã‚‚å…¥ã£ã¦ãªã„
                    zip_code=article.xpath(".//span[3]/text()").get().strip(),
                    address=article.xpath(".//span[4]/text()").get().strip(),
                    tel=article.xpath(".//span[5]/text()").get(),
                    official_page=article.xpath(".//span[6]/a/@href").get(),
                )

                yield item
