import re
import scrapy
from logzero import logger
from goto_eat_scrapy.items import ShopItem
from goto_eat_scrapy.exceptions import ScrapingError

class OsakaSpider(scrapy.Spider):
    """
    usage:
      $ scrapy crawl osaka -O output.csv
    """
    name = 'osaka'
    allowed_domains = [ 'goto-eat.weare.osaka-info.jp' ] # weareã£ã¦è¦ã‚‹ï¼Ÿ

    # https://goto-eat.weare.osaka-info.jp/gotoeat/ ã‹ã‚‰ã€Œã™ã¹ã¦ã®ã‚¨ãƒªã‚¢ã€ã€Œã™ã¹ã¦ã®ã‚¸ãƒ£ãƒ³ãƒ«ã€ã§æ¤œç´¢ã—ãŸçµæœã€‚ã¤ã‚ˆã„(ç¢ºä¿¡)
    start_urls = ['https://goto-eat.weare.osaka-info.jp/gotoeat/?search_element_0_0=2&search_element_0_1=3&search_element_0_2=4&search_element_0_3=5&search_element_0_4=6&search_element_0_5=7&search_element_0_6=8&search_element_0_7=9&search_element_0_8=10&search_element_0_9=11&search_element_0_cnt=10&search_element_1_cnt=17&search_element_2_cnt=1&s_keyword_3=&cf_specify_key_3_0=gotoeat_shop_address01&cf_specify_key_3_1=gotoeat_shop_address02&cf_specify_key_3_2=gotoeat_shop_address03&cf_specify_key_length_3=2&searchbutton=%E5%8A%A0%E7%9B%9F%E5%BA%97%E8%88%97%E3%82%92%E6%A4%9C%E7%B4%A2%E3%81%99%E3%82%8B&csp=search_add&feadvns_max_line_0=4&fe_form_no=0']

    # ã‚¸ãƒ£ãƒ³ãƒ«ã¯ã‚¿ã‚°ã§ç®¡ç†ã•ã‚Œã¦ã‚‹ãŒã€åœ°åŸŸå(æ³‰å·ã¨ã‹)ã‚‚ä¸€ç·’ã«ã‚¿ã‚°ç®¡ç†ã•ã‚Œã¦ã¦åŒºåˆ¥ã§ããªã„ã®ã§â€¦
    genre_list = ['å±…é…’å±‹', 'å’Œé£Ÿ', 'å¯¿å¸ãƒ»é­šæ–™ç†', 'ã†ã©ã‚“ãƒ»ãã°', 'é‹', 'ãŠå¥½ã¿ç„¼ããƒ»ãŸã“ç„¼ã', 'ãƒ©ãƒ¼ãƒ¡ãƒ³ãƒ»ã¤ã‘éºº',
        'éƒ·åœŸæ–™ç†', 'æ´‹é£Ÿãƒ»è¥¿æ´‹æ–™ç†', 'ã‚«ãƒ¬ãƒ¼', 'ç„¼è‚‰ãƒ»ãƒ›ãƒ«ãƒ¢ãƒ³', 'ã‚¤ã‚¿ãƒªã‚¢ãƒ³ãƒ»ãƒ•ãƒ¬ãƒ³ãƒ', 'ä¸­è¯æ–™ç†', 'ã‚¢ã‚¸ã‚¢ãƒ»ã‚¨ã‚¹ãƒ‹ãƒƒã‚¯æ–™ç†',
        'ã‚«ãƒ•ã‚§ãƒ»ã‚¹ã‚¤ãƒ¼ãƒ„', 'ãƒ›ãƒ†ãƒ«ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³', 'ãã®ä»–']
    area_list = ['ã‚­ã‚¿', 'ãƒŸãƒŠãƒŸ', 'å¤§é˜ªåŸ', 'ã‚ã¹ã®ãƒ»å¤©ç‹å¯º', 'ãƒ™ã‚¤ã‚¨ãƒªã‚¢', 'åŒ—æ‘‚', 'åŒ—æ²³å†…', 'ä¸­æ²³å†…','å—æ²³å†…', 'æ³‰å·']

    def parse(self, response):
        # å„åŠ ç›Ÿåº—æƒ…å ±ã‚’æŠ½å‡º
        base = '//div[@class="search_result_box"]/ul'
        for i, _ in enumerate(response.xpath(f'{base}/li'), 1):
            item = ShopItem()
            # ã€Œåº—èˆ—åã€ (ä¾‹: "ãªã‹å¯ã€€ãƒã‚¤ãƒ‘ãƒ¼ã‚¢ãƒ­ãƒ¼æ¾ãƒ¶ä¸˜åº—")
            # MEMO: ç‰ç‰ä¸Šæ–°åº„é§…å‰åº—ã ã‘åº—åå…¥ã£ã¦ãªã„ã‚“ã™ã‘ã©â€¦
            item['shop_name'] = response.xpath(f'{base}/li[{i}]/p[@class="name"]/text()').extract_first()
            # ã€Œã‚¸ãƒ£ãƒ³ãƒ«åã€ (ä¾‹: "ãã®ä»–")
            for tag in response.xpath(f'{base}/li[{i}]/ul[@class="tag_list"]/li/text()').extract():
                # å®Œå…¨ä¸€è‡´ã™ã‚‹ã‚¿ã‚°ãŒã‚ã‚Œã°ãã‚Œã‚’è¨­å®š(ãã‚Œä»¥å¤–ã®ã‚¿ã‚°ã«ã¤ã„ã¦ã¯åœ°åŸŸåã¨ã—ã¦skip)
                # MEMO: ä¸€ã¤ã®åº—ã«è¤‡æ•°ã®ã‚¸ãƒ£ãƒ³ãƒ«ã‚¿ã‚°ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯æƒ³å®šã—ã¦ã„ãªã„(å¤§é˜ªã®å ´åˆã¯ãªã„â€¦ã¯ãšâ€¦ï¼Ÿ)
                # ä»–ã®è‡ªæ²»ä½“ã§ã¯è¦‹ã‹ã‘ãŸã‚ˆã†ãªæ°—ãŒã™ã‚‹ã®ã§ã€ãã‚“ã¨ãã¯ã©ã†ã™ã‚‹ã‹ãªâ€¦ (CSVã®ã‚«ãƒ©ãƒ å†…ã§æ‹¡å¼µã‹ãªâ€¦)
                if tag in self.genre_list:
                    item['genre_name'] = tag
                    break
                if tag not in self.area_list:
                    raise ScrapingError(f'æƒ³å®šã—ã¦ã„ãªã„ã€ä¸æ˜ãªã‚¿ã‚° ã€Œ{tag}ã€')
            # tableã‚¿ã‚°éƒ¨åˆ†ã®æŠ½å‡º
            item = self.__parse_table(f'{base}/li[{i}]', response, item)
            # MEMO: è©³ç´°ãƒšãƒ¼ã‚¸ã¾ã§å›ã›ã°å…¬å¼ãƒšãƒ¼ã‚¸ã®URLãŒå–ã‚Œã‚‹ãŒã€ãã‚Œã ã‘ã®ãŸã‚ã«ã‚„ã‚‹ã‹â€¦ï¼Ÿã£ã¦æ°—ãŒã—ã¦ããŸã®ã§ã—ãªã„
            # 2020/11/11ç¾åœ¨ã§819ãƒšãƒ¼ã‚¸ã‚‚ã‚ã‚‹ã—â€¦
            yield item

        # ã€Œ>ã€ãƒœã‚¿ãƒ³ãŒãªã‘ã‚Œã°(æœ€çµ‚ãƒšãƒ¼ã‚¸ãªã®ã§)çµ‚äº†
        next_page = response.xpath('//div[@role="navigation"]//a[@rel="next"]/@href').extract_first()
        if next_page is None:
            logger.info('ğŸ’» finished. last page = ' + response.request.url)
            return

        logger.info(f'ğŸ›« next url = {next_page}')

        yield scrapy.Request(next_page, callback=self.parse)

    def __parse_table(self, base: str, response, item: ShopItem):
        """
        tableã‚¿ã‚°éƒ¨åˆ†ã®é …ç›®ã€‚ãƒ†ãƒ¼ãƒ–ãƒ«ä¸­ã®é …ç›®ãŒå€‹åˆ¥ã«è‰²ã€…ã‚ã‚‹ã®ã§é©å½“ã«å‡¦ç†
        """
        # å¯å¤‰ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã«ã¯ãªã£ã¦ãªã„(ã¨æ€ã†)ã®ã§å¤šåˆ†4è¡Œè¶…ãˆã‚‹ã“ã¨ã¯ãªã„(ã¯ãš)
        for i in range(1, 4):
            try:
                # théƒ¨åˆ†ã‚’keyåã¨ã—ã¦å‡¦ç†(ç”Ÿhtmlã«tbodyãŒãªã„ã®ã§æ³¨æ„)
                key = response.xpath(f'{base}/table/tr[{i}]/th/text()').extract_first()
                if key is None:
                    break
                key = key.strip()
                if key == 'ä½æ‰€':
                    text = response.xpath(f'{base}/table/tr[{i}]/td/text()').extract()
                    item['zip_code'] = text[0].strip()
                    item['address'] = re.sub(r'\s', '', text[1])
                elif key == 'TEL':
                    item['tel'] = response.xpath(f'{base}/table/tr[{i}]/td/text()').extract_first()
                elif key == 'å–¶æ¥­æ™‚é–“':
                    item['opening_hours'] = response.xpath(f'{base}/table/tr[{i}]/td/text()').extract_first()
                elif key == 'å®šä¼‘æ—¥':
                    item['closing_day'] = response.xpath(f'{base}/table/tr[{i}]/td/text()').extract_first()
                else:
                    # ãƒ†ãƒ¼ãƒ–ãƒ«ä¸­ã«ä¸Šè¨˜ä»¥å¤–ã®é …ç›®åãŒå‡ºãŸã‚‰
                    text = response.xpath(f'{base}/table/tr[{i}]/td/text()').extract_first().strip()
                    raise ScrapingError(f'ä¸æ˜ãªãƒ†ãƒ¼ãƒ–ãƒ«é …ç›®: {key} : {text}')
            except Exception as e:
                logger.error("ğŸ’€ğŸ’€ğŸ’€ [ERR!!] parse Error: " + key)
                raise e

        return item
