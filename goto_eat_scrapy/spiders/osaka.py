import re
import scrapy
from goto_eat_scrapy.items import ShopItem
from goto_eat_scrapy.spiders.abstract import AbstractSpider

class OsakaSpider(AbstractSpider):
    """
    usage:
      $ scrapy crawl osaka -O osaka.csv
    """
    name = 'osaka'
    allowed_domains = [ 'goto-eat.weare.osaka-info.jp' ] # weareã£ã¦è¦ã‚‹ï¼Ÿ

    # MEMO: https://goto-eat.weare.osaka-info.jp/gotoeat/ ã‹ã‚‰ã€Œã™ã¹ã¦ã®ã‚¨ãƒªã‚¢ã€ã€Œã™ã¹ã¦ã®ã‚¸ãƒ£ãƒ³ãƒ«ã€ã§æ¤œç´¢ã—ãŸçµæœã€‚ã¤ã‚ˆã„
    start_urls = ['https://goto-eat.weare.osaka-info.jp/gotoeat/?search_element_0_0=2&search_element_0_1=3&search_element_0_2=4&search_element_0_3=5&search_element_0_4=6&search_element_0_5=7&search_element_0_6=8&search_element_0_7=9&search_element_0_8=10&search_element_0_9=11&search_element_0_cnt=10&search_element_1_cnt=17&search_element_2_cnt=1&s_keyword_3=&cf_specify_key_3_0=gotoeat_shop_address01&cf_specify_key_3_1=gotoeat_shop_address02&cf_specify_key_3_2=gotoeat_shop_address03&cf_specify_key_length_3=2&searchbutton=%E5%8A%A0%E7%9B%9F%E5%BA%97%E8%88%97%E3%82%92%E6%A4%9C%E7%B4%A2%E3%81%99%E3%82%8B&csp=search_add&feadvns_max_line_0=4&fe_form_no=0']

    # ã‚¸ãƒ£ãƒ³ãƒ«ã¯ã‚¿ã‚°ã§ç®¡ç†ã•ã‚Œã¦ã‚‹ãŒã€åœ°åŸŸå(æ³‰å·ã¨ã‹)ã‚‚ä¸€ç·’ã«ã‚¿ã‚°ç®¡ç†ã•ã‚Œã¦ã„ã‚‹ã®ã§
    area_list = [
        'ã‚­ã‚¿',
        'ãƒŸãƒŠãƒŸ',
        'å¤§é˜ªåŸ',
        'ã‚ã¹ã®ãƒ»å¤©ç‹å¯º',
        'ãƒ™ã‚¤ã‚¨ãƒªã‚¢',
        'åŒ—æ‘‚',
        'åŒ—æ²³å†…',
        'ä¸­æ²³å†…',
        'å—æ²³å†…',
        'æ³‰å·',
    ]

    def parse(self, response):
        # å„åŠ ç›Ÿåº—æƒ…å ±ã‚’æŠ½å‡º
        self.logzero_logger.info(f'ğŸ’¾ url = {response.request.url}')
        for li in response.xpath('//div[@class="search_result_box"]/ul/li'):
            item = ShopItem()
            # MEMO: 2020/11/28ç¾åœ¨ã€ç‰ç‰ä¸Šæ–°åº„é§…å‰åº—ã ã‘åº—åãŒå–ã‚Œãªã„ã€æ–‡å­—ã‚³ãƒ¼ãƒ‰é–¢ä¿‚ï¼Ÿ
            item['shop_name'] = li.xpath('.//p[@class="name"]/text()').get()

            # ã‚¸ãƒ£ãƒ³ãƒ«å
            # MEMO: è¤‡æ•°ã‚¸ãƒ£ãƒ³ãƒ«ã‚’æƒ³å®šã—ãŸå®Ÿè£…ã ãŒã€å¤§é˜ªã®å ´åˆã¯è¤‡æ•°ã‚¸ãƒ£ãƒ³ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹ã®ãƒ‡ãƒ¼ã‚¿ã¯ãªã„(ã¯ãš)
            genres = []
            for tag in li.xpath('.//ul[@class="tag_list"]/li/text()'):
                tagtext = tag.get().strip()
                if tagtext in self.area_list:
                    # åœ°åã‚¿ã‚°ã®å ´åˆã¯skip
                    continue
                genres.append(tagtext)
            item['genre_name'] = '|'.join(genres)

            text = li.xpath('.//table/tr[1]/td/text()').getall()
            item['zip_code'] = text[0].strip()
            item['address'] = re.sub(r'\s', '', text[1])
            item['tel'] = li.xpath('.//table/tr[2]/td/text()').get()
            item['opening_hours'] = li.xpath('.//table/tr[3]/td/text()').get()
            item['closing_day'] = li.xpath('.//table/tr[4]/td/text()').get()

            # MEMO: è©³ç´°ãƒšãƒ¼ã‚¸ã¾ã§å›ã›ã°å…¬å¼ãƒšãƒ¼ã‚¸ã®URLãŒå–ã‚Œã‚‹ãŒã€ãã‚Œã ã‘ã®ãŸã‚ã«15kä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã¹ãã¨ã¯
            # æ€ãˆãªã‹ã£ãŸã®ã§ã‚„ã£ã¦ãªã„ã€‚å¤§é˜ªã®å ´åˆã€2020/11/28ç¾åœ¨ã§ã‚‚881ãƒšãƒ¼ã‚¸ã‚‚ã‚ã‚‹ã—â€¦
            self.logzero_logger.debug(item)
            yield item

        # ã€Œ>ã€ãƒœã‚¿ãƒ³ãŒãªã‘ã‚Œã°(æœ€çµ‚ãƒšãƒ¼ã‚¸ãªã®ã§)çµ‚äº†
        next_page = response.xpath('//div[@role="navigation"]//a[@rel="next"]/@href').extract_first()
        if next_page is None:
            self.logzero_logger.info('ğŸ’» finished. last page = ' + response.request.url)
            return

        self.logzero_logger.info(f'ğŸ›« next url = {next_page}')

        yield scrapy.Request(next_page, callback=self.parse)
