import re
import scrapy
from goto_eat_scrapy.items import ShopItem
from goto_eat_scrapy.spiders.abstract import AbstractSpider

class OsakaSpider(AbstractSpider):
    """
    usage:
      $ scrapy crawl osaka -O osaka.csv
    """
    name = 'osaka'
    allowed_domains = [ 'goto-eat.weare.osaka-info.jp' ] # weareã£ã¦è¦ã‚‹ï¼Ÿ

    # MEMO: https://goto-eat.weare.osaka-info.jp/gotoeat/ ã‹ã‚‰ã€Œã™ã¹ã¦ã®ã‚¨ãƒªã‚¢ã€ã€Œã™ã¹ã¦ã®ã‚¸ãƒ£ãƒ³ãƒ«ã€ã§æ¤œç´¢ã—ãŸçµæœã€‚ã¤ã‚ˆã„
    start_urls = ['https://goto-eat.weare.osaka-info.jp/gotoeat/?search_element_0_0=2&search_element_0_1=3&search_element_0_2=4&search_element_0_3=5&search_element_0_4=6&search_element_0_5=7&search_element_0_6=8&search_element_0_7=9&search_element_0_8=10&search_element_0_9=11&search_element_0_cnt=10&search_element_1_cnt=17&search_element_2_cnt=1&s_keyword_3=&cf_specify_key_3_0=gotoeat_shop_address01&cf_specify_key_3_1=gotoeat_shop_address02&cf_specify_key_3_2=gotoeat_shop_address03&cf_specify_key_length_3=2&searchbutton=%E5%8A%A0%E7%9B%9F%E5%BA%97%E8%88%97%E3%82%92%E6%A4%9C%E7%B4%A2%E3%81%99%E3%82%8B&csp=search_add&feadvns_max_line_0=4&fe_form_no=0']

    # ã‚¸ãƒ£ãƒ³ãƒ«ã¯ã‚¿ã‚°ã§ç®¡ç†ã•ã‚Œã¦ã‚‹ãŒã€ã‚¨ãƒªã‚¢åã‚‚ä¸€ç·’ã«ã‚¿ã‚°æ‰±ã„ã¨ãªã£ã¦ã„ã‚‹ãŸã‚
    area_list = [
        'ã‚­ã‚¿',
        'ãƒŸãƒŠãƒŸ',
        'å¤§é˜ªåŸ',
        'ã‚ã¹ã®ãƒ»å¤©ç‹å¯º',
        'ãƒ™ã‚¤ã‚¨ãƒªã‚¢',
        'åŒ—æ‘‚',
        'åŒ—æ²³å†…',
        'ä¸­æ²³å†…',
        'å—æ²³å†…',
        'æ³‰å·',
    ]

    # MEMO: ç¨€ã«504 Gateway Time-outã«ãªã‚‹ã®ã§ã€DELAYã‚’å¤šã‚ã«è¨­å®šã—ã¦æ§˜å­è¦‹
    # ãŸã ã—å¤§é˜ªã¯ãã‚‚ãã‚‚ä»¶æ•°ãŒå¤šã„ã®ã§ã€ã‚ã¾ã‚Šå¤šãã—ã™ãã‚‹ã¨æ™‚é–“ãŒã‹ã‹ã£ã¦ã—ã¾ã†
    custom_settings = {
        'DOWNLOAD_DELAY': 4,
    }

    def parse(self, response):
        # å„åŠ ç›Ÿåº—æƒ…å ±ã‚’æŠ½å‡º
        self.logzero_logger.info(f'ğŸ’¾ url = {response.request.url}')
        for article in response.xpath('//div[@class="search_result_box"]/ul/li'):
            item = ShopItem()

            shop_name = article.xpath('.//p[@class="name"]/text()').get()
            # MEMO: 2020/11/28ç¾åœ¨ã€ã€Œç‰ç‰ä¸Šæ–°åº„é§…å‰åº—ã€ã ã‘åº—åãŒå–ã‚Œãªã„ãŸã‚ä¾‹å¤–å¯¾å¿œã€‚
            # og:titleè¾ºã‚Šã«ã¯å‡ºã¦ãŠã‚Šã€ä»–ã®ã€Œç‰ç‰ã€ã§ã¯æ™®é€šã«è¡¨ç¤ºã•ã‚Œã‚‹åº—ã‚‚ã‚ã‚‹ã®ã§ã€è¬â€¦
            # (äº¬æ©‹åº—ã¯ã²ã‚‰ãŒãªã§ã€Œã¿ã‚“ã¿ã‚“ã€ã¨ãªã£ã¦ãŠã‚Šã€è¬ã¯æ·±ã¾ã£ã¦ã„ã‚‹)
            item['shop_name'] = shop_name.strip() if shop_name else 'ç‰ç‰ä¸Šæ–°åº„é§…å‰åº—'

            # ã‚¸ãƒ£ãƒ³ãƒ«å
            # MEMO: è¤‡æ•°ã‚¸ãƒ£ãƒ³ãƒ«ã‚’æƒ³å®šã—ãŸå®Ÿè£…ã ãŒã€å¤§é˜ªã®å ´åˆã¯è¤‡æ•°ã‚¸ãƒ£ãƒ³ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã¯ãªã„(ã¯ãš)
            genres = []
            for tag in article.xpath('.//ul[@class="tag_list"]/li/text()'):
                tagtext = tag.get().strip()
                if tagtext in self.area_list:
                    # MEMEO: åœ°åã‚¿ã‚°ã‚‚è¤‡æ•°æŒ‡å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã¯ãªã„å‰æ(å¾Œå‹ã¡ã§ä¸Šæ›¸ãã•ã‚Œã‚‹)
                    item['area_name'] = tagtext
                    continue
                genres.append(tagtext)
            item['genre_name'] = '|'.join(genres)

            # MEMO: å–¶æ¥­æ™‚é–“ã€å®šä¼‘æ—¥ã®æœªå…¥åŠ›ãŒã‚ã‚‹ã®ã§ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’ç›´æ¥æŒ‡å®š
            text = article.xpath('.//table/tr[1]/td/text()').getall()
            item['zip_code'] = text[0].strip()
            item['address'] = re.sub(r'\s', '', text[1])
            item['tel'] = article.xpath('.//table/tr[2]/td/text()').get()
            item['opening_hours'] = article.xpath('.//table/tr[3]/td/text()').get()
            item['closing_day'] = article.xpath('.//table/tr[4]/td/text()').get()

            # MEMO: è©³ç´°ãƒšãƒ¼ã‚¸ã¾ã§å›ã›ã°å…¬å¼ãƒšãƒ¼ã‚¸ã®URLãŒå–ã‚Œã‚‹ãŒã€ãã‚Œã ã‘ã®ãŸã‚ã«15kä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã¹ãã¨ã¯
            # æ€ãˆãªã‹ã£ãŸã®ã§ã‚„ã£ã¦ãªã„ã€‚å¤§é˜ªã®å ´åˆã€2020/11/28ç¾åœ¨ã§ã‚‚881ãƒšãƒ¼ã‚¸ã‚‚ã‚ã‚‹ã—â€¦
            item['detail_page'] = article.xpath('.//a[contains(text(), "è©³ã—ãè¦‹ã‚‹")]/@href').get().strip()

            yield item

        # ã€Œ>ã€ãƒœã‚¿ãƒ³ãŒãªã‘ã‚Œã°(æœ€çµ‚ãƒšãƒ¼ã‚¸ãªã®ã§)çµ‚äº†
        next_page = response.xpath('//div[@role="navigation"]//a[@rel="next"]/@href').extract_first()
        if next_page is None:
            self.logzero_logger.info('ğŸ’» finished. last page = ' + response.request.url)
            return

        self.logzero_logger.info(f'ğŸ›« next url = {next_page}')

        yield scrapy.Request(next_page, callback=self.parse)
