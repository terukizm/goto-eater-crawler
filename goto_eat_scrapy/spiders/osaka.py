import re
import scrapy
from logzero import logger
from goto_eat_scrapy.items import ShopItem
from goto_eat_scrapy.exceptions import ScrapingError

class OsakaSpider(scrapy.Spider):
    """
    usage:
      $ scrapy crawl osaka -O output.csv
    """
    name = 'osaka'
    allowed_domains = [ 'goto-eat.weare.osaka-info.jp' ] # weareã£ã¦è¦ã‚‹ï¼Ÿ

    # https://goto-eat.weare.osaka-info.jp/gotoeat/ ã‹ã‚‰ã€Œã™ã¹ã¦ã®ã‚¨ãƒªã‚¢ã€ã€Œã™ã¹ã¦ã®ã‚¸ãƒ£ãƒ³ãƒ«ã€ã§æ¤œç´¢ã—ãŸçµæœã€‚ã¤ã‚ˆã„(ç¢ºä¿¡)
    start_urls = ['https://goto-eat.weare.osaka-info.jp/gotoeat/?search_element_0_0=2&search_element_0_1=3&search_element_0_2=4&search_element_0_3=5&search_element_0_4=6&search_element_0_5=7&search_element_0_6=8&search_element_0_7=9&search_element_0_8=10&search_element_0_9=11&search_element_0_cnt=10&search_element_1_cnt=17&search_element_2_cnt=1&s_keyword_3=&cf_specify_key_3_0=gotoeat_shop_address01&cf_specify_key_3_1=gotoeat_shop_address02&cf_specify_key_3_2=gotoeat_shop_address03&cf_specify_key_length_3=2&searchbutton=%E5%8A%A0%E7%9B%9F%E5%BA%97%E8%88%97%E3%82%92%E6%A4%9C%E7%B4%A2%E3%81%99%E3%82%8B&csp=search_add&feadvns_max_line_0=4&fe_form_no=0']

    # ã‚¸ãƒ£ãƒ³ãƒ«ã¯ã‚¿ã‚°ã§ç®¡ç†ã•ã‚Œã¦ã‚‹ãŒã€åœ°åŸŸå(æ³‰å·ã¨ã‹)ã‚‚ä¸€ç·’ã«ã‚¿ã‚°ç®¡ç†ã•ã‚Œã¦ã¦åŒºåˆ¥ã§ããªã„ã®ã§â€¦
    genre_list = [
        'å±…é…’å±‹',
        'å’Œé£Ÿ',
        'å¯¿å¸ãƒ»é­šæ–™ç†',
        'ã†ã©ã‚“ãƒ»ãã°',
        'é‹',
        'ãŠå¥½ã¿ç„¼ããƒ»ãŸã“ç„¼ã',
        'ãƒ©ãƒ¼ãƒ¡ãƒ³ãƒ»ã¤ã‘éºº',
        'éƒ·åœŸæ–™ç†',
        'æ´‹é£Ÿãƒ»è¥¿æ´‹æ–™ç†',
        'ã‚«ãƒ¬ãƒ¼',
        'ç„¼è‚‰ãƒ»ãƒ›ãƒ«ãƒ¢ãƒ³',
        'ã‚¤ã‚¿ãƒªã‚¢ãƒ³ãƒ»ãƒ•ãƒ¬ãƒ³ãƒ',
        'ä¸­è¯æ–™ç†',
        'ã‚¢ã‚¸ã‚¢ãƒ»ã‚¨ã‚¹ãƒ‹ãƒƒã‚¯æ–™ç†',
        'ã‚«ãƒ•ã‚§ãƒ»ã‚¹ã‚¤ãƒ¼ãƒ„',
        'ãƒ›ãƒ†ãƒ«ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³',
        'ãã®ä»–',
    ]
    area_list = [
        'ã‚­ã‚¿',
        'ãƒŸãƒŠãƒŸ',
        'å¤§é˜ªåŸ',
        'ã‚ã¹ã®ãƒ»å¤©ç‹å¯º',
        'ãƒ™ã‚¤ã‚¨ãƒªã‚¢',
        'åŒ—æ‘‚',
        'åŒ—æ²³å†…',
        'ä¸­æ²³å†…',
        'å—æ²³å†…',
        'æ³‰å·',
    ]

    def parse(self, response):
        # å„åŠ ç›Ÿåº—æƒ…å ±ã‚’æŠ½å‡º
        for li in response.xpath('//div[@class="search_result_box"]/ul/li'):
            item = ShopItem()
            # ã€Œåº—èˆ—åã€ (ä¾‹: "ãªã‹å¯ã€€ãƒã‚¤ãƒ‘ãƒ¼ã‚¢ãƒ­ãƒ¼æ¾ãƒ¶ä¸˜åº—")
            item['shop_name'] = li.xpath('.//p[@class="name"]/text()').get() # MEMO: ç‰ç‰ä¸Šæ–°åº„é§…å‰åº—ã ã‘åº—åå…¥ã£ã¦ãªã„ã‚“ã™ã‘ã©â€¦
            # ã€Œã‚¸ãƒ£ãƒ³ãƒ«åã€ (ä¾‹: "ãã®ä»–")
            for tag in li.xpath('.//ul[@class="tag_list"]/li/text()'):
                # å®Œå…¨ä¸€è‡´ã™ã‚‹ã‚¿ã‚°ãŒã‚ã‚Œã°è¨­å®š(ãã‚Œä»¥å¤–ã®ã‚¿ã‚°ã¯åœ°åŸŸåã¨ã—ã¦skip)
                # MEMO: ä¸€ã¤ã®åº—ã«è¤‡æ•°ã®ã‚¸ãƒ£ãƒ³ãƒ«ã‚¿ã‚°ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯æƒ³å®šã—ã¦ã„ãªã„(å¤§é˜ªã®å ´åˆã¯ãªã„â€¦ã¯ãšâ€¦ï¼Ÿ)
                # ä»–ã®è‡ªæ²»ä½“ã§ã¯è¦‹ã‹ã‘ãŸã‚ˆã†ãªæ°—ãŒã™ã‚‹ã®ã§ã€ãã‚“ã¨ãã¯ã©ã†ã™ã‚‹ã‹ãªâ€¦ (CSVã®ã‚«ãƒ©ãƒ å†…ã§æ‹¡å¼µã‹ãªâ€¦)
                tagtext = tag.get()
                if tagtext in self.genre_list:
                    item['genre_name'] = tagtext
                    break
                if tagtext not in self.area_list:
                    raise ScrapingError(f'æƒ³å®šã—ã¦ã„ãªã„ã€ä¸æ˜ãªã‚¿ã‚° ã€Œ{tagtext}ã€')
            # tableã‚¿ã‚°éƒ¨åˆ†ã®æŠ½å‡º
            # ã€Œä½æ‰€ã€
            text = li.xpath('.//table/tr[1]/td/text()').getall()
            item['zip_code'] = text[0].strip()
            item['address'] = re.sub(r'\s', '', text[1])
            # ã€ŒTELã€
            item['tel'] = li.xpath('.//table/tr[2]/td/text()').get()
            # ã€Œå–¶æ¥­æ™‚é–“ã€
            item['opening_hours'] = li.xpath('.//table/tr[3]/td/text()').get()
            # ã€Œå®šä¼‘æ—¥ã€
            item['closing_day'] = li.xpath('.//table/tr[4]/td/text()').get()
            # MEMO: è©³ç´°ãƒšãƒ¼ã‚¸ã¾ã§å›ã›ã°å…¬å¼ãƒšãƒ¼ã‚¸ã®URLãŒå–ã‚Œã‚‹ãŒã€ãã‚Œã ã‘ã®ãŸã‚ã«ã‚„ã‚‹ã‹â€¦ï¼Ÿã£ã¦æ°—ãŒã—ã¦ããŸã®ã§ã‚„ã£ã¦ãªã„
            # 2020/11/11ç¾åœ¨ã§819ãƒšãƒ¼ã‚¸ã‚‚ã‚ã‚‹ã—â€¦
            yield item

        # ã€Œ>ã€ãƒœã‚¿ãƒ³ãŒãªã‘ã‚Œã°(æœ€çµ‚ãƒšãƒ¼ã‚¸ãªã®ã§)çµ‚äº†
        next_page = response.xpath('//div[@role="navigation"]//a[@rel="next"]/@href').extract_first()
        if next_page is None:
            logger.info('ğŸ’» finished. last page = ' + response.request.url)
            return

        logger.info(f'ğŸ›« next url = {next_page}')

        yield scrapy.Request(next_page, callback=self.parse)
