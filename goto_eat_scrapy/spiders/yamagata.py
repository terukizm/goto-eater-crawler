import re
import scrapy
import json
from logzero import logger

from goto_eat_scrapy.items import ShopItem

class YamagataSpider(scrapy.Spider):
    """
    usage:
      $ scrapy crawl yamagata -O 06_yamagata.csv
    """
    name = 'yamagata'
    allowed_domains = [ 'yamagata-gotoeat.com' ]    # .comã¨ã¯

    endpoint = 'https://yamagata-gotoeat.com/wp/wp-content/themes/gotoeat/search.php'

    area_list = [
        'å±±å½¢å¸‚',
        'å¯’æ²³æ±Ÿå¸‚',
        'ä¸Šå±±å¸‚',
        'æ‘å±±å¸‚',
        'å¤©ç«¥å¸‚',
        'æ±æ ¹å¸‚',
        'å°¾èŠ±æ²¢å¸‚',
        'å±±è¾ºç”º',
        'ä¸­å±±ç”º',
        'æ²³åŒ—ç”º',
        'è¥¿å·ç”º',
        'æœæ—¥ç”º',
        'å¤§æ±Ÿç”º',
        'å¤§çŸ³ç”°ç”º',
        'æ–°åº„å¸‚',
        'é‡‘å±±ç”º',
        'æœ€ä¸Šç”º',
        'èˆŸå½¢ç”º',
        'çœŸå®¤å·ç”º',
        'å¤§è”µæ‘',
        'é®­å·æ‘',
        'æˆ¸æ²¢æ‘',
        'ç±³æ²¢å¸‚',
        'å—é™½å¸‚',
        'é•·äº•å¸‚',
        'é«˜ç• ç”º',
        'å·è¥¿ç”º',
        'å°å›½ç”º',
        'ç™½é·¹ç”º',
        'é£¯è±Šç”º',
        'é…’ç”°å¸‚',
        'é¶´å²¡å¸‚',
        'ä¸‰å·ç”º',
        'åº„å†…ç”º',
        'éŠä½ç”º',
    ]

    genre_list = [
        'ãƒ©ãƒ¼ãƒ¡ãƒ³',
        'ã†ã©ã‚“ãƒ»ãã°',
        'ã‚«ãƒ¬ãƒ¼',
        'å±…é…’å±‹ãƒ»å‰µä½œæ–™ç†',
        'ç„¼é³¥ãƒ»ä¸²æšã’',
        'ãƒ€ã‚¤ãƒ‹ãƒ³ã‚°ãƒãƒ¼ãƒ»ãƒãƒ«',
        'å’Œé£Ÿãƒ»å¯¿å¸ãƒ»å¤©ã·ã‚‰',
        'é‰„æ¿ãƒ»ã‚¹ãƒ†ãƒ¼ã‚­',
        'æ´‹é£Ÿ',
        'ã‚¤ã‚¿ãƒªã‚¢ãƒ³',
        'ãƒ•ãƒ¬ãƒ³ãƒ',
        'ä¸­è¯æ–™ç†',
        'ç„¼è‚‰',
        'ã‚¢ã‚¸ã‚¢ãƒ»ã‚¨ã‚¹ãƒ‹ãƒƒã‚¯',
        'ãŠå¥½ã¿ç„¼ããƒ»ã‚‚ã‚“ã˜ã‚ƒ',
        'ã‚«ãƒ•ã‚§ãƒ»ã‚¹ã‚¤ãƒ¼ãƒ„',
        'ãƒ•ã‚¡ãƒŸãƒªãƒ¼ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³',
        'ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆãƒ•ãƒ¼ãƒ‰',
        'å®šé£Ÿå±‹',
        'ãã®ä»–',
    ]

    def start_requests(self):
        params = {'text': '', 'page': '1'}
        yield scrapy.FormRequest(self.endpoint, callback=self.parse, method='POST', formdata=params)


    def parse(self, response):
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¯jsonãªã®ã§ç›´æ¥parse
        data = json.loads(response.body)

        # (å‚è€ƒ): htmlã¯ä»¥ä¸‹ã®DOMæ§‹é€ ã«ã—ã¦ã‹ã‚‰ã€XPathã§æŠ½å‡º
        #
        # <article>
        #   <li>
        #     <ul class="search__result__tag">
        #       <li>é¶´å²¡å¸‚</li>
        #       <li>å’Œé£Ÿãƒ»å¯¿å¸ãƒ»å¤©ã·ã‚‰</li>
        #     </ul>
        #     <h2>å’Œé£Ÿè—¤å·</h2>
        #     <div>997-0034 å±±å½¢çœŒé¶´å²¡å¸‚æœ¬ç”º2-15-27</div>
        #     <div>TEL : 023-522-8821</div>
        #   </li>
        #   <li>....<li>
        #   <li>....<li>
        # </article>

        html = scrapy.Selector(text=f'<article>{data["html"]}</article>')
        for article in html.xpath('//article/li'):
            item = ShopItem()
            item['shop_name'] = article.xpath('.//h2/text() | .//h2/a/text()').get().strip()

            place = article.xpath('.//div[1]/text()').get().strip()
            m = re.match(r'(?P<zip_code>.*?)\s(?P<address>.*)', place)
            item['address'] = m.group('address')
            item['zip_code'] = m.group('zip_code')

            tel = article.xpath('.//div[2]/text()').get()
            item['tel'] = tel.replace('TEL : ', '') if tel else None

            # ã€Œã‚¸ãƒ£ãƒ³ãƒ«åã€ (ä¾‹: "ãã®ä»–")
            for tag in article.xpath('.//ul[@class="search__result__tag"]/li/text()'):
                # å®Œå…¨ä¸€è‡´ã™ã‚‹ã‚¿ã‚°ãŒã‚ã‚Œã°è¨­å®š(ãã‚Œä»¥å¤–ã®ã‚¿ã‚°ã¯åœ°åŸŸåã¨ã—ã¦skip)
                tagtext = tag.get()
                if not tagtext:
                    continue
                if tagtext in self.genre_list:
                    item['genre_name'] = tagtext
                    break
                if tagtext not in self.area_list:
                    raise ScrapingError(f'æƒ³å®šã—ã¦ã„ãªã„ã€ä¸æ˜ãªã‚¿ã‚° ã€Œ{tagtext}ã€')

            yield item

        # æœ€å¾Œã®ãƒšãƒ¼ã‚¸ã‚’è¡¨ç¤ºã•ã›ã¦ã‚‚ã€Œæ¬¡ã¸(æœ€å¾Œã¸)ã€ã®å‡ºã—åˆ†ã‘ãŒã•ã‚Œã¦ãªã„ã®ã§ã€
        # ã€Œactiveã®ãƒšãƒ¼ã‚¸=æ¬¡ã¸ã§è¡¨ç¤ºã•ã›ã‚‹ãƒšãƒ¼ã‚¸ã€ã¨ãªã£ãŸã¨ãã«çµ‚äº†
        pager = scrapy.Selector(text=data["pager"])
        active_page = pager.xpath('//div[@class="search__pager"]/ul/li[@class="search__pager__link active"]/@data-page').get()
        next_page = pager.xpath('//div[@class="search__pager"]/div[contains(text(),"æ¬¡ã¸")]/@data-page').get()

        # (å‚è€ƒ): pagerã¯ä»¥ä¸‹ã®DOMæ§‹é€ 
        #
        # <div class="search__pager">
        #   <div class="search__pager__link seach__pager__small" data-page="1">æœ€åˆã¸</div>
        #   <div class="search__pager__link seach__pager__btn" data-page="1">å‰ã¸</div>
        #   <ul>
        #     <li>1</li>
        #     <li class="search__pager__link active" data-page="2">2</li>
        #     <li>...</li>
        #   </ul>
        #   <div class="search__pager__link search__pager__btn" data-page="3">æ¬¡ã¸</div>
        #   <div class="search__pager__link seach__pager__small" data-page="85">æœ€å¾Œã¸</div>
        # </div>

        if active_page == next_page:
            logger.info('ğŸ’» finished. last page = ' + active_page)
            return

        logger.info(f'next_page = {next_page}')
        params = {'text': '', 'page': next_page}
        yield scrapy.FormRequest(self.endpoint, callback=self.parse, method='POST', formdata=params)
