import re
import scrapy
import json
from goto_eat_scrapy.items import ShopItem
from goto_eat_scrapy.spiders.abstract import AbstractSpider

class YamagataSpider(AbstractSpider):
    """
    usage:
      $ scrapy crawl yamagata -O yamagata.csv
    """
    name = 'yamagata'
    allowed_domains = [ 'yamagata-gotoeat.com' ]    # .comã¨ã¯
    endpoint = 'https://yamagata-gotoeat.com/wp/wp-content/themes/gotoeat/search.php'

    area_list = [
        'å±±å½¢å¸‚',
        'å¯’æ²³æ±Ÿå¸‚',
        'ä¸Šå±±å¸‚',
        'æ‘å±±å¸‚',
        'å¤©ç«¥å¸‚',
        'æ±æ ¹å¸‚',
        'å°¾èŠ±æ²¢å¸‚',
        'å±±è¾ºç”º',
        'ä¸­å±±ç”º',
        'æ²³åŒ—ç”º',
        'è¥¿å·ç”º',
        'æœæ—¥ç”º',
        'å¤§æ±Ÿç”º',
        'å¤§çŸ³ç”°ç”º',
        'æ–°åº„å¸‚',
        'é‡‘å±±ç”º',
        'æœ€ä¸Šç”º',
        'èˆŸå½¢ç”º',
        'çœŸå®¤å·ç”º',
        'å¤§è”µæ‘',
        'é®­å·æ‘',
        'æˆ¸æ²¢æ‘',
        'ç±³æ²¢å¸‚',
        'å—é™½å¸‚',
        'é•·äº•å¸‚',
        'é«˜ç• ç”º',
        'å·è¥¿ç”º',
        'å°å›½ç”º',
        'ç™½é·¹ç”º',
        'é£¯è±Šç”º',
        'é…’ç”°å¸‚',
        'é¶´å²¡å¸‚',
        'ä¸‰å·ç”º',
        'åº„å†…ç”º',
        'éŠä½ç”º',
    ]

    def start_requests(self):
        params = {'text': '', 'page': '1'}
        yield scrapy.FormRequest(self.endpoint, callback=self.parse, method='POST', formdata=params)

    def parse(self, response):
        self.logzero_logger.info(f'ğŸ’¾ url = {response.request.url}')

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¯jsonãªã®ã§ç›´æ¥parse
        # (å‚è€ƒ): htmlã¯ä»¥ä¸‹ã®DOMæ§‹é€ ã«ã—ã¦ã‹ã‚‰ã€XPathã§æŠ½å‡º
        #
        # <article>
        #   <li>
        #     <ul class="search__result__tag">
        #       <li>é¶´å²¡å¸‚</li>
        #       <li>å’Œé£Ÿãƒ»å¯¿å¸ãƒ»å¤©ã·ã‚‰</li>
        #     </ul>
        #     <h2>å’Œé£Ÿè—¤å·</h2>
        #     <div>997-0034 å±±å½¢çœŒé¶´å²¡å¸‚æœ¬ç”º2-15-27</div>
        #     <div>TEL : 023-522-8821</div>
        #   </li>
        #   <li>....<li>
        #   <li>....<li>
        # </article>
        data = json.loads(response.body)
        html = scrapy.Selector(text='<article>{}</article>'.format(data["html"]))
        for article in html.xpath('//article/li'):
            item = ShopItem()
            item['shop_name'] = article.xpath('.//h2/text() | .//h2/a/text()').get().strip()

            place = article.xpath('.//div[1]/text()').get().strip()
            m = re.match(r'(?P<zip_code>.*?)\s(?P<address>.*)', place)
            item['address'] = m.group('address')
            item['zip_code'] = m.group('zip_code')

            tel = article.xpath('.//div[2]/text()').get()
            item['tel'] = tel.replace('TEL : ', '') if tel else None

            # ã€Œã‚¸ãƒ£ãƒ³ãƒ«åã€ (ã‚¨ãƒªã‚¢åã®ã‚¿ã‚°ã¯skip)
            for tag in article.xpath('.//ul[@class="search__result__tag"]/li/text()'):
                tagtext = tag.get()
                if not tagtext:
                    continue
                if tagtext in self.area_list:
                    continue
                item['genre_name'] = tagtext

            self.logzero_logger.debug(item)
            yield item

        # æœ€å¾Œã®ãƒšãƒ¼ã‚¸ã‚’è¡¨ç¤ºã•ã›ã¦ã‚‚ã€Œæ¬¡ã¸(æœ€å¾Œã¸)ã€ã®å‡ºã—åˆ†ã‘ãŒã•ã‚Œã¦ãªã„ã®ã§ã€
        # ã€Œactiveã®ãƒšãƒ¼ã‚¸=æ¬¡ã¸ã§è¡¨ç¤ºã•ã›ã‚‹ãƒšãƒ¼ã‚¸ã€ã¨ãªã£ãŸã¨ãã«çµ‚äº†
        pager = scrapy.Selector(text=data["pager"])
        active_page = pager.xpath('//div[@class="search__pager"]/ul/li[@class="search__pager__link active"]/@data-page').get()
        next_page = pager.xpath('//div[@class="search__pager"]/div[contains(text(),"æ¬¡ã¸")]/@data-page').get()

        # (å‚è€ƒ): pagerã¯ä»¥ä¸‹ã®DOMæ§‹é€ 
        #
        # <div class="search__pager">
        #   <div class="search__pager__link seach__pager__small" data-page="1">æœ€åˆã¸</div>
        #   <div class="search__pager__link seach__pager__btn" data-page="1">å‰ã¸</div>
        #   <ul>
        #     <li>1</li>
        #     <li class="search__pager__link active" data-page="2">2</li>
        #     <li>...</li>
        #   </ul>
        #   <div class="search__pager__link search__pager__btn" data-page="3">æ¬¡ã¸</div>
        #   <div class="search__pager__link seach__pager__small" data-page="85">æœ€å¾Œã¸</div>
        # </div>

        if active_page == next_page:
            self.logzero_logger.info('ğŸ’» finished. last page = ' + active_page)
            return

        self.logzero_logger.info(f'next_page = {next_page}')
        params = {'text': '', 'page': next_page}
        yield scrapy.FormRequest(self.endpoint, callback=self.parse, method='POST', formdata=params)
