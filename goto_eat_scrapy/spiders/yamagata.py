import json
import re

import scrapy

from goto_eat_scrapy.items import ShopItem
from goto_eat_scrapy.spiders.abstract import AbstractSpider


class YamagataSpider(AbstractSpider):
    """
    usage:
      $ scrapy crawl yamagata -O yamagata.csv
    """

    name = "yamagata"
    allowed_domains = ["yamagata-gotoeat.com"]

    endpoint = "https://yamagata-gotoeat.com/wp/wp-content/themes/gotoeat/search.php"

    area_list = [
        "å±±å½¢å¸‚",
        "å¯’æ²³æ±Ÿå¸‚",
        "ä¸Šå±±å¸‚",
        "æ‘å±±å¸‚",
        "å¤©ç«¥å¸‚",
        "æ±æ ¹å¸‚",
        "å°¾èŠ±æ²¢å¸‚",
        "å±±è¾ºç”º",
        "ä¸­å±±ç”º",
        "æ²³åŒ—ç”º",
        "è¥¿å·ç”º",
        "æœæ—¥ç”º",
        "å¤§æ±Ÿç”º",
        "å¤§çŸ³ç”°ç”º",
        "æ–°åº„å¸‚",
        "é‡‘å±±ç”º",
        "æœ€ä¸Šç”º",
        "èˆŸå½¢ç”º",
        "çœŸå®¤å·ç”º",
        "å¤§è”µæ‘",
        "é®­å·æ‘",
        "æˆ¸æ²¢æ‘",
        "ç±³æ²¢å¸‚",
        "å—é™½å¸‚",
        "é•·äº•å¸‚",
        "é«˜ç• ç”º",
        "å·è¥¿ç”º",
        "å°å›½ç”º",
        "ç™½é·¹ç”º",
        "é£¯è±Šç”º",
        "é…’ç”°å¸‚",
        "é¶´å²¡å¸‚",
        "ä¸‰å·ç”º",
        "åº„å†…ç”º",
        "éŠä½ç”º",
    ]

    def start_requests(self):
        params = {"text": "", "page": "1"}
        yield scrapy.FormRequest(self.endpoint, callback=self.parse, method="POST", formdata=params)

    def parse(self, response):
        self.logzero_logger.info(f"ğŸ’¾ url = {response.request.url}")

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¯jsonãªã®ã§ç›´æ¥parse
        # (å‚è€ƒ): "html"ã«<li>...</li>ãŒç›´æ¥å…¥ã£ã¦ã„ã‚‹ã®ã§ã€ä»¥ä¸‹ã®DOMæ§‹é€ ã«å¤‰æ›ã—ã¦ã‹ã‚‰ã€XPathã‚’ä½¿ã£ã¦æŠ½å‡º
        #
        # <article>
        #   <li>
        #     <ul class="search__result__tag">
        #       <li>é¶´å²¡å¸‚</li>
        #       <li>å’Œé£Ÿãƒ»å¯¿å¸ãƒ»å¤©ã·ã‚‰</li>
        #     </ul>
        #     <h2>å’Œé£Ÿè—¤å·</h2>    # å…¬å¼ãƒšãƒ¼ã‚¸ãŒaã‚¿ã‚°ã§å…¥ã‚‹å ´åˆãŒã‚ã‚‹
        #     <div>997-0034 å±±å½¢çœŒé¶´å²¡å¸‚æœ¬ç”º2-15-27</div>
        #     <div>TEL : 023-522-8821</div>
        #   </li>
        #   <li>....<li>
        #   <li>....<li>
        # </article>
        data = json.loads(response.body)
        html = scrapy.Selector(text="<article>{}</article>".format(data["html"]))
        for article in html.xpath("//article/li"):
            item = ShopItem()
            item["shop_name"] = article.xpath(".//h2/text() | .//h2/a/text()").get().strip()
            item["official_page"] = article.xpath(".//h2/a/@href").get()

            place = article.xpath(".//div[1]/text()").get().strip()
            m = re.match(r"(?P<zip_code>.*?)\s(?P<address>.*)", place)

            if m:
                item["address"] = m.group("address")
                item["zip_code"] = m.group("zip_code")
            else:
                item["address"] = place
                item["zip_code"] = None # MEMO: 2021/08/19 "ã‚¨ãƒãƒ†ãƒ¼ã‚«"ã«éƒµä¾¿ç•ªå·ãŒãªã„

            tel = article.xpath(".//div[2]/text()").get()
            item["tel"] = tel.replace("TEL : ", "") if tel else None

            # ã‚¿ã‚°(ã‚¸ãƒ£ãƒ³ãƒ«åã€ã‚¨ãƒªã‚¢å)
            # MEMO: ã‚¸ãƒ£ãƒ³ãƒ«åã€ã‚¨ãƒªã‚¢åã¨ã‚‚ã«è¤‡æ•°æŒ‡å®šã¯ãªã„ã¨ã„ã†å‰æã§ã®å®Ÿè£…
            for tag in article.xpath('.//ul[@class="search__result__tag"]/li/text()'):
                tagtext = tag.get()
                if not tagtext:
                    continue
                if tagtext in self.area_list:
                    item["area_name"] = tagtext
                    continue
                item["genre_name"] = tagtext

            yield item

        # ä¸€èˆ¬çš„ãªãƒšãƒ¼ã‚¸ãƒ£å®Ÿè£…ã®ã‚ˆã†ã«æœ€çµ‚ãƒšãƒ¼ã‚¸ã‚’è¡¨ç¤ºã•ã›ãŸã¨ãã«ã€Œæ¬¡ã¸(æœ€å¾Œã¸)ã€ãƒªãƒ³ã‚¯ãŒå‡ºãªã„ã‚ˆã†ã«ãªã£ã¦ãªã„ã®ã§ã€
        # activeã®ãƒšãƒ¼ã‚¸=ã€Œæ¬¡ã¸ã€ãƒªãƒ³ã‚¯ã®ãƒšãƒ¼ã‚¸ã€€ã¨ãªã£ãŸã¨ãã«çµ‚äº†
        pager = scrapy.Selector(text=data["pager"])
        active_page = pager.xpath(
            '//div[@class="search__pager"]/ul/li[@class="search__pager__link active"]/@data-page'
        ).get()
        next_page = pager.xpath('//div[@class="search__pager"]/div[contains(text(),"æ¬¡ã¸")]/@data-page').get()

        # (å‚è€ƒ): pagerã¯ä»¥ä¸‹ã®DOMæ§‹é€ 
        #
        # <div class="search__pager">
        #   <div class="search__pager__link seach__pager__small" data-page="1">æœ€åˆã¸</div>
        #   <div class="search__pager__link seach__pager__btn" data-page="1">å‰ã¸</div>
        #   <ul>
        #     <li>1</li>
        #     <li class="search__pager__link active" data-page="2">2</li>
        #     <li>...</li>
        #   </ul>
        #   <div class="search__pager__link search__pager__btn" data-page="3">æ¬¡ã¸</div>
        #   <div class="search__pager__link seach__pager__small" data-page="85">æœ€å¾Œã¸</div>
        # </div>

        if active_page == next_page:
            self.logzero_logger.info("ğŸ’» finished. last page = " + active_page)
            return

        self.logzero_logger.info(f"next_page = {next_page}")
        params = {"text": "", "page": next_page}
        yield scrapy.FormRequest(self.endpoint, callback=self.parse, method="POST", formdata=params)
