import re

import scrapy

from goto_eat_scrapy.items import ShopItem
from goto_eat_scrapy.spiders.abstract import AbstractSpider


class NiigataSpider(AbstractSpider):
    """
    usage:
      $ scrapy crawl niigata -O niigata.csv
    """

    name = "niigata"
    allowed_domains = ["niigata-gte.com"]
    start_urls = ["https://niigata-gte.com/shop/"]

    area_list = [
        "æ–°æ½Ÿå¸‚åŒ—åŒº",
        "æ–°æ½Ÿå¸‚æ±åŒº",
        "æ–°æ½Ÿå¸‚ä¸­å¤®åŒº",
        "æ–°æ½Ÿå¸‚æ±Ÿå—åŒº",
        "æ–°æ½Ÿå¸‚ç§‹è‘‰åŒº",
        "æ–°æ½Ÿå¸‚å—åŒº",
        "æ–°æ½Ÿå¸‚è¥¿åŒº",
        "æ–°æ½Ÿå¸‚è¥¿è’²åŒº",
        "é•·å²¡å¸‚",
        "ä¸‰æ¡å¸‚",
        "æŸå´å¸‚",
        "æ–°ç™ºç”°å¸‚",
        "å°åƒè°·å¸‚",
        "åŠ èŒ‚å¸‚",
        "åæ—¥ç”ºå¸‚",
        "è¦‹é™„å¸‚",
        "æ‘ä¸Šå¸‚",
        "ç‡•å¸‚",
        "ç³¸é­šå·å¸‚",
        "å¦™é«˜å¸‚",
        "äº”æ³‰å¸‚",
        "ä¸Šè¶Šå¸‚",
        "é˜¿è³€é‡å¸‚",
        "ä½æ¸¡å¸‚",
        "é­šæ²¼å¸‚",
        "å—é­šæ²¼å¸‚",
        "èƒå†…å¸‚",
        "è–ç± ç”º",
        "å¼¥å½¦æ‘",
        "ç”°ä¸Šç”º",
        "é˜¿è³€ç”º",
        "å‡ºé›²å´ç”º",
        "æ¹¯æ²¢ç”º",
        "æ´¥å—ç”º",
        "åˆˆç¾½æ‘",
        "é–¢å·æ‘",
        "ç²Ÿå³¶æµ¦æ‘",
    ]

    def parse(self, response):
        self.logzero_logger.info(f"ğŸ’¾ url = {response.request.url}")
        for article in response.xpath('//div[@id="result"]/div[@class="cont"]'):
            item = ShopItem()
            item["shop_name"] = "".join(article.xpath(".//h4/text() | .//h4/a/text()").getall()).strip()
            item["official_page"] = article.xpath(".//h4/a/@href").get()

            place = "".join(article.xpath('.//p[@class="add"]/text()').getall()).strip()
            m = re.match(r"ã€’(?P<zip_code>.*?)\s(?P<address>.*)", place)
            item["address"] = m.group("address")
            item["zip_code"] = m.group("zip_code")
            item["tel"] = article.xpath('.//p[@class="tel"]/text()').get()

            # æ–°æ½ŸçœŒã¯ã€Œåœ°åŸŸåã€ã¨ã€Œã‚¸ãƒ£ãƒ³ãƒ«åã€ãŒã‚¿ã‚°ã§ä¸€ç·’ã«ãªã£ã¦ã„ã‚‹ãŸã‚ã€ã‚¸ãƒ£ãƒ³ãƒ«åã ã‘ã‚’å–å¾—
            genres = []
            for tag in article.xpath('.//div[@class="tag"]/span/text()'):
                tagtext = tag.get().strip()
                if not tagtext:
                    continue
                if tagtext in self.area_list:
                    # MEMO: åœ°åŸŸåã‚¿ã‚°ã¯è¤‡æ•°æŒ‡å®šã•ã‚Œã¦ã„ãªã„å‰æ(ã‚‚ã—ã‚ã‚Œã°ã€å¾Œå‹ã¡ã§ä¸Šæ›¸ãã•ã‚Œã¦ã—ã¾ã†ã®ã§æ³¨æ„)
                    item["area_name"] = tagtext
                    continue
                genres.append(tagtext)
            item["genre_name"] = "|".join(genres)

            gmap_url = article.xpath('.//p[@class="add"]/span/a/@href').get() or ""
            m = re.search("\/@(?P<lat>\d+\.\d+)\,(?P<lng>\d+\.\d+)\,", gmap_url)
            if m:
                item["provided_lat"] = m.group("lat")
                item["provided_lng"] = m.group("lng")

            yield item

        # ã€Œæ¬¡ã¸ã€ãƒœã‚¿ãƒ³ãŒãªã‘ã‚Œã°(æœ€çµ‚ãƒšãƒ¼ã‚¸ãªã®ã§)çµ‚äº†
        next_page = response.xpath('//div[@id="pagination"]/ul/li[@class="next"]/a/@onclick').extract_first()
        if next_page is None:
            self.logzero_logger.info("ğŸ’» finished. last page = " + response.request.url)
            return

        m = re.match(r"^mySubmit\('(?P<page>.*)'\);$", next_page)
        next_page = m.group("page")
        self.logzero_logger.info(f"ğŸ›« next url = {next_page}")

        yield scrapy.Request(next_page, callback=self.parse)
