import re
import scrapy
from logzero import logger
from goto_eat_scrapy.items import ShopItem

class NiigataSpider(scrapy.Spider):
    """
    usage:
      $ scrapy crawl niigata -O 15_niigata.csv
    """
    name = 'niigata'
    allowed_domains = [ 'niigata-gte.com' ]     # .comã¨ã¯

    start_urls = ['https://niigata-gte.com/shop/']

    # ã‚¸ãƒ£ãƒ³ãƒ«ã¯ã‚¿ã‚°ã§ç®¡ç†ã•ã‚Œã¦ã‚‹ãŒã€åœ°åŸŸå(æ³‰å·ã¨ã‹)ã‚‚ä¸€ç·’ã«ã‚¿ã‚°ç®¡ç†ã•ã‚Œã¦ã¦åŒºåˆ¥ã§ããªã„ã®ã§â€¦
    genre_list = [
        'å’Œé£Ÿ',
        'å¯¿å¸',
        'å‰²çƒ¹',
        'æ´‹é£Ÿ',
        'ã‚¤ã‚¿ãƒªã‚¢ãƒ³',
        'ãƒ•ãƒ¬ãƒ³ãƒ',
        'ä¸­è¯',
        'ãƒ©ãƒ¼ãƒ¡ãƒ³',
        'ãƒ•ã‚¡ã‚¹ãƒˆãƒ•ãƒ¼ãƒ‰',
        'è»½é£Ÿ',
        'å–«èŒ¶',
        'å±…é…’å±‹',
        'ãã®ä»–',
    ]
    area_list = [
        'æ–°æ½Ÿå¸‚åŒ—åŒº',
        'æ–°æ½Ÿå¸‚æ±åŒº',
        'æ–°æ½Ÿå¸‚ä¸­å¤®åŒº',
        'æ–°æ½Ÿå¸‚æ±Ÿå—åŒº',
        'æ–°æ½Ÿå¸‚ç§‹è‘‰åŒº',
        'æ–°æ½Ÿå¸‚å—åŒº',
        'æ–°æ½Ÿå¸‚è¥¿åŒº',
        'æ–°æ½Ÿå¸‚è¥¿è’²åŒº',
        'é•·å²¡å¸‚',
        'ä¸‰æ¡å¸‚',
        'æŸå´å¸‚',
        'æ–°ç™ºç”°å¸‚',
        'å°åƒè°·å¸‚',
        'åŠ èŒ‚å¸‚',
        'åæ—¥ç”ºå¸‚',
        'è¦‹é™„å¸‚',
        'æ‘ä¸Šå¸‚',
        'ç‡•å¸‚',
        'ç³¸é­šå·å¸‚',
        'å¦™é«˜å¸‚',
        'äº”æ³‰å¸‚',
        'ä¸Šè¶Šå¸‚',
        'é˜¿è³€é‡å¸‚',
        'ä½æ¸¡å¸‚',
        'é­šæ²¼å¸‚',
        'å—é­šæ²¼å¸‚',
        'èƒå†…å¸‚',
        'è–ç± ç”º',
        'å¼¥å½¦æ‘',
        'ç”°ä¸Šç”º',
        'é˜¿è³€ç”º',
        'å‡ºé›²å´ç”º',
        'æ¹¯æ²¢ç”º',
        'æ´¥å—ç”º',
        'åˆˆç¾½æ‘',
        'é–¢å·æ‘',
        'ç²Ÿå³¶æµ¦æ‘',
    ]

    def parse(self, response):
        # å„åŠ ç›Ÿåº—æƒ…å ±ã‚’æŠ½å‡º
        for article in response.xpath('//div[@id="result"]/div[@class="cont"]'):
            item = ShopItem()
            item['shop_name'] = ''.join(article.xpath('.//h4/text() | .//h4/a/text()').getall()).strip()
            item['offical_page'] = article.xpath('.//h4/a/@href').get()

            place = ''.join(article.xpath('.//p[@class="add"]/text()').getall()).strip()
            m = re.match(r'ã€’(?P<zip_code>.*?)\s(?P<address>.*)', place)
            item['address'] = m.group('address')
            item['zip_code'] = m.group('zip_code')
            item['tel'] = article.xpath('.//p[@class="tel"]/text()').get()

            # ã€Œã‚¸ãƒ£ãƒ³ãƒ«åã€ (ä¾‹: "ãã®ä»–")
            for tag in article.xpath('.//div[@class="tag"]/span/text()'):
                # å®Œå…¨ä¸€è‡´ã™ã‚‹ã‚¿ã‚°ãŒã‚ã‚Œã°è¨­å®š(ãã‚Œä»¥å¤–ã®ã‚¿ã‚°ã¯åœ°åŸŸåã¨ã—ã¦skip)
                tagtext = tag.get()
                if not tagtext:
                    continue
                if tagtext in self.genre_list:
                    item['genre_name'] = tagtext
                    break
                if tagtext not in self.area_list:
                    raise ScrapingError(f'æƒ³å®šã—ã¦ã„ãªã„ã€ä¸æ˜ãªã‚¿ã‚° ã€Œ{tagtext}ã€')

            yield item

        # ã€ŒNEXTã€ãƒœã‚¿ãƒ³ãŒãªã‘ã‚Œã°(æœ€çµ‚ãƒšãƒ¼ã‚¸ãªã®ã§)çµ‚äº†
        next_page = response.xpath('//div[@id="pagination"]/ul/li[@class="next"]/a/@onclick').extract_first()
        if next_page is None:
            logger.info('ğŸ’» finished. last page = ' + response.request.url)
            return

        m = re.match(r"^mySubmit\('(?P<page>.*)'\);$", next_page)
        next_page = m.group('page')
        logger.info(f'ğŸ›« next url = {next_page}')

        yield scrapy.Request(next_page, callback=self.parse)
