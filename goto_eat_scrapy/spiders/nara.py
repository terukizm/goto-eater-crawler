import re
import scrapy
import pathlib
import pandas as pd
from goto_eat_scrapy import settings
from goto_eat_scrapy.items import ShopItem
from goto_eat_scrapy.spiders.abstract import AbstractSpider

class naraSpider(AbstractSpider):
    """
    usage:
      $ scrapy crawl nara -O nara.csv
    """
    name = 'nara'
    allowed_domains = [ 'premium-gift.jp' ]
    start_urls = ['https://premium-gift.jp/nara-eat']

    def parse(self, response):
        # Topãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’å–ã‚‹(å¤šåˆ†æœ€æ–°ç‰ˆãŒæœ€åˆã«æ¥ã‚‹ã ã‚ã†ã¨ã„ã†æ±ºã‚æ‰“ã¡)
        xlsx_url = response.xpath('//section[@class="news-list"]//a[contains(text(), "Excelå½¢å¼")]/@href').extract_first()
        yield scrapy.Request(xlsx_url, callback=self.parse_from_xlsx)

    def parse_from_xlsx(self, response):
        cache_dir = pathlib.Path(__file__).parent.parent.parent / '.scrapy' / settings.HTTPCACHE_DIR / self.name
        tmp_xlsx = str(cache_dir / 'åˆ©ç”¨åº—èˆ—ä¸€è¦§.xlsx')

        with open(tmp_xlsx, 'wb') as f:
            f.write(response.body)
        self.logzero_logger.info(f'ğŸ’¾ saved pdf: {response.request.url} > {tmp_xlsx}')

        df = pd.read_excel(tmp_xlsx, sheet_name='ãƒªã‚¹ãƒˆ').fillna({'é›»è©±ç•ªå·': '', 'URL': ''})
        for _, row in df.iterrows():
            item = ShopItem()
            item['area_name'] = row['ã‚¨ãƒªã‚¢'].strip()
            item['shop_name'] = ' '.join(row['åº—èˆ—åç§°'].splitlines()) # åº—èˆ—åã«æ”¹è¡ŒãŒå…¥ã£ã¦ã‚‹ã‚‚ã®ãŒã‚ã‚‹ã®ã§åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã«ç½®æ›
            item['genre_name'] = row['ã‚«ãƒ†ã‚´ãƒªãƒ¼'].strip()
            item['address'] = row['ä½æ‰€'].strip()
            item['tel'] = row['é›»è©±ç•ªå·']
            item['official_page'] = row['URL']

            self.logzero_logger.debug(item)
            yield item
