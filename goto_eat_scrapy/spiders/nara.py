import pathlib

import pandas as pd
import scrapy

from goto_eat_scrapy import settings
from goto_eat_scrapy.items import ShopItem
from goto_eat_scrapy.spiders.abstract import AbstractSpider


class naraSpider(AbstractSpider):
    """
    usage:
      $ scrapy crawl nara -O nara.csv
    """

    name = "nara"
    allowed_domains = ["premium-gift.jp"]

    # MEMO: åˆ©ç”¨åº—èˆ—ä¸€è¦§ã®ä¸­ã‹ã‚‰é©å½“ã«ãƒªãƒ³ã‚¯ãŒè²¼ã‚‰ã‚Œã¦ã„ã‚‹
    # @see https://premium-gift.jp/nara-eat/use_store
    start_urls = ["https://premium-gift.jp/nara-eat/use_store/detail?id=215783"]

    CACHE_PATH = pathlib.Path(__file__).parent.parent.parent / ".scrapy" / settings.HTTPCACHE_DIR / name
    CACHE_PATH.mkdir(parents=True, exist_ok=True)

    def parse(self, response):
        xlsx_url = response.xpath(
            '//table[@class="common-table"]/tbody/tr/th[contains(text(), "URL")]/following-sibling::td/a/@href'
        ).extract_first()
        yield scrapy.Request(xlsx_url, callback=self.parse_from_xlsx)

    def parse_from_xlsx(self, response):
        tmp_xlsx = str(self.CACHE_PATH / "åˆ©ç”¨åº—èˆ—ä¸€è¦§.xlsx")

        with open(tmp_xlsx, "wb") as f:
            f.write(response.body)
            self.logzero_logger.info(f"ğŸ’¾ saved xlsx: {response.request.url} > {tmp_xlsx}")

        df = pd.read_excel(tmp_xlsx, sheet_name="ãƒªã‚¹ãƒˆ", dtype=str).fillna({"åº—èˆ—_TEL": "", "åº—èˆ—_URL": ""})
        for _, row in df.iterrows():
            item = ShopItem()
            item["area_name"] = row["ã‚¨ãƒªã‚¢"].strip()
            item["shop_name"] = row["åº—èˆ—_åç§°"]  # MEMO: åº—èˆ—åã«æ”¹è¡ŒãŒå…¥ã£ã¦ã‚‹ã‚‚ã®ãŒã‚ã‚‹
            item["genre_name"] = row["ã‚«ãƒ†ã‚´ãƒªãƒ¼"].strip()
            item["address"] = row["åº—èˆ—_ä½æ‰€"].strip()
            item["tel"] = row["åº—èˆ—_TEL"]
            item["official_page"] = row["åº—èˆ—_URL"]

            yield item
