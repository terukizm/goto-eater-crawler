
import scrapy
import w3lib
import pathlib
import pandas as pd
from goto_eat_scrapy import settings
from goto_eat_scrapy.items import ShopItem
from goto_eat_scrapy.spiders.abstract import AbstractSpider

class AkitaSpider(AbstractSpider):
    """
    usage:
      $ scrapy crawl akita -O akita.csv
    """
    name = 'akita'
    allowed_domains = [ 'gotoeat-akita.com' ]    # .comã¨ã¯
    start_urls = ['https://gotoeat-akita.com/csv/list.csv']

    def __init__(self, logfile=None, *args, **kwargs):
        super().__init__(logfile, *args, **kwargs)

    def parse(self, response):
        # MEMO: tempfile, io.stringIOç­‰ã§ã¯pd.read_csv()ãŒãã¡ã‚“ã¨å‹•ä½œã—ãªã‹ã£ãŸã®ã§
        # scrapyã®httpcacheã¨åŒã˜å ´æ‰€(settings.HTTPCACHE_DIR)ã«æ›¸ãè¾¼ã‚“ã§ã„ã‚‹
        cache_dir = pathlib.Path.cwd() / '.scrapy' / settings.HTTPCACHE_DIR / self.name
        tmp_csv = str(cache_dir / 'list.csv')
        with open(tmp_csv, 'wb') as f:
            f.write(response.body)
        self.logzero_logger.info(f'ğŸ’¾ saved csv: {response.request.url} > {tmp_csv}')

        df = pd.read_csv(tmp_csv, header=None, \
            names=('åº—èˆ—å', 'å¸‚ç”ºæ‘', 'æ‰€åœ¨åœ°', 'é›»è©±ç•ªå·', 'å…¬å¼ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸')
        ).fillna({'å…¬å¼ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸': ''})

        for _, row in df.iterrows():
            item = ShopItem()
            # CSVä¸­ã« <!-- --> å½¢å¼ã§æ¤œç´¢ç”¨(?)ã®ãµã‚ŠãŒãª/ãƒ•ãƒªã‚¬ãƒŠãŒå…¥ã£ã¦ã„ã‚‹ã®ã§å‰Šé™¤
            item['shop_name'] = w3lib.html.remove_tags(row['åº—èˆ—å']).strip()

            # åŒã˜ãæ¤œç´¢ç”¨(?)ã®æ–‡å­—åˆ—ãŒå…¥ã£ã¦ã„ã‚‹ã‚‚ã®ãŒã‚ã‚‹ãŒã€ã“ã¡ã‚‰ã®å…¥åŠ›å€¤ã¯åˆ©ç”¨ã™ã‚‹
            # (ç”³è«‹æ™‚ã«æœªå…¥åŠ›ã ã£ãŸé …ç›®ã‚’æ‰‹ä½œæ¥­ã§åŸ‹ã‚ã¦ã‚‹ï¼Ÿ)
            item['address'] = row['æ‰€åœ¨åœ°'].replace('<!--', '').replace('-->', '').strip()

            item['tel'] = row['é›»è©±ç•ªå·']
            item['offical_page'] = row['å…¬å¼ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸']
            item['genre_name'] = None    # ç§‹ç”°ã«ã¯ã‚¸ãƒ£ãƒ³ãƒ«æƒ…å ±ãªã—

            self.logzero_logger.debug(item)
            yield item
