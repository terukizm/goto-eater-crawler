import scrapy

from goto_eat_scrapy.items import ShopItem
from goto_eat_scrapy.spiders.abstract import AbstractSpider


class EhimeSpider(AbstractSpider):
    """
    usage:
      $ scrapy crawl ehime -O ehime.csv
    """

    name = "ehime"
    allowed_domains = ["goto-eat-ehime.com"]
    start_urls = ["https://www.goto-eat-ehime.com/shop_list/"]

    def parse(self, response):
        self.logzero_logger.info(f"ğŸ’¾ url = {response.request.url}")
        for article in response.xpath('//div[@id="sortable"]/ul[@class="shop_list"]/li'):
            item = ShopItem()
            item["shop_name"] = article.xpath(".//div/dl/dt/text()").get().strip()
            item["genre_name"] = article.xpath(".//div/p/span/text()").get().strip()
            item["address"] = (
                article.xpath('.//div/dl/dd/ul/li/span[contains(text(), "ä½æ‰€")]/following-sibling::span/text()')
                .get()
                .strip()
            )
            item["tel"] = article.xpath('.//div/dl/dd/ul/li/span/a[@class="tel_link"]/text()').get()

            item["detail_page"] = article.xpath('.//p[@class="btn_link"]/a/@href').get().strip()
            # MEMO: closing_day, opening_hours, official_pageãªã©ã‚’è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—å¯èƒ½ã ãŒã€ã¨ã‚Šã‚ãˆãšæœªå¯¾å¿œ
            # ã‚¨ãƒªã‚¢ã«ã¤ã„ã¦ã¯æ¤œç´¢æ¡ä»¶ã§ã®ã¿è¨­å®šå¯èƒ½ãªãŸã‚ã€çµæœ(ä¸€è¦§/è©³ç´°)ãƒšãƒ¼ã‚¸ã‹ã‚‰ã¯å–å¾—ä¸å¯ã ãŒã€å±±å£çœŒã¨åŒæ§˜ã®æ–¹å¼ã§å¯¾å¿œå¯èƒ½

            yield item

        # ã€Œ>ã€ãƒœã‚¿ãƒ³ãŒãªã‘ã‚Œã°(æœ€çµ‚ãƒšãƒ¼ã‚¸ãªã®ã§)çµ‚äº†
        next_page = response.xpath('//div[@role="navigation"]/a[@rel="next"]/@href').extract_first()
        if next_page is None:
            self.logzero_logger.info("ğŸ’» finished. last page = " + response.request.url)
            return

        self.logzero_logger.info(f"ğŸ›« next url = {next_page}")
        yield scrapy.Request(next_page, callback=self.parse)
