import asyncio
from pyppeteer import launch
from pyppeteer.errors import PageError
import lxml.html
import pathlib
import pandas as pd
from logzero import logger
from goto_eat_scrapy import settings
from goto_eat_scrapy.items import ShopItem

async def crawl():
    browser = await launch({
        'defaultViewport': None,
        'headless': True,           # pipenvãªã©æ‰‹å…ƒã§å‹•ã‹ã—ã¦ã„ã‚‹å ´åˆã¯Falseã«ã™ã‚‹ã¨å®Ÿéš›ã«chroniumãŒå‹•ãã®ã§ã‚ã‹ã‚Šã‚„ã™ã„
        'args': ['--no-sandbox'],   # Dockerå†…ã§å‹•ã‹ã™å ´åˆã«å¿…è¦(ã‚ã‚“ã¾ã‚Šã‚ˆã‚ã—ããªã„ã‚‰ã—ã„ãŒ)
        'slowMo': 5,                # é©å®œé£Ÿã‚ã›ãªã„ã¨ã‚³ã‚±ã‚‹(ï¼Ÿ)ã®ã§
    })
    page = await browser.newPage()
    await page.goto('https://oita-gotoeat.com/shop/')

    try:
        # é©å½“ã«waitå…¥ã‚Œã¤ã¤ç„¡é™ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã®ã€Œã‚‚ã£ã¨è¦‹ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’é€£æ‰“
        while True:
            await page.evaluate("""{window.scrollBy(0, document.body.scrollHeight);}""")
            await page.waitFor(1000);
            await page.click('input[class="more"]')
    except PageError:
        # FIXME: ã‚¯ã‚½å®Ÿè£…ã€æ¬¡ãƒšãƒ¼ã‚¸ãŒãªããªã£ãŸã‚‰ãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯ãŒã§ããšã€PageErrorãŒraiseã•ã‚Œã‚‹ã®ã§ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
        pass

    html: str = await page.content()
    await browser.close()

    if not html:
        raise Exception('html is none....')

    return html


def parse(html: str):
    """
    lxmlã‚’ä½¿ã£ã¦xpathãƒ™ãƒ¼ã‚¹ã§parse
    """
    results = []
    response = lxml.html.fromstring(html)
    for article in response.xpath('//li[@class="box-sh cf"]'):
        item = ShopItem()
        item['genre_name'] = article.xpath('.//div[@class="tag cf"]/p[@class="genre"]/span/text()')[0].strip()
        item['shop_name'] = article.xpath('.//p[@class="name"]/text()')[0].strip()
        item['address'] = article.xpath('.//div[@class="first"]/p[@class="add"]/text()')[0].strip()
        tel = article.xpath('.//div[@class="second"]/p[@class="s-call"]/span[@class="shoptel"]/a/text()')
        item['tel'] = tel[0].strip() if tel else None

        logger.debug(item)
        results.append(item)

    return results

def main(outfile: str):
    # ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã¯æ™‚é–“ã‹ã‹ã‚‹ã®ã§ä¸€å›žæˆåŠŸã—ãŸã‚‰pickleã«ã—ã¦ã‚‹
    # ä¿å­˜å…ˆã¯scrapyã®httpcacheã¨åŒã˜å ´æ‰€(settings.HTTPCACHE_DIR)
    # TODO: ã“ã®è¾ºã®cacheå‡¦ç†ã‚’æ¶ˆã™ã€ã‚‚ã—ãã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ã™ã‚‹
    cache_dir = pathlib.Path.cwd() / '.scrapy' / settings.HTTPCACHE_DIR / 'oita_script'
    cache_dir.mkdir(parents=True, exist_ok=True)
    _html_pkl = str(cache_dir / 'pyppeteer.pkl')
    try:
        logger.info('  load from pickle ...')
        html = pd.read_pickle(_html_pkl)
    except FileNotFoundError:
        logger.info('  crawling ...')
        html = asyncio.get_event_loop().run_until_complete(crawl())
        pd.to_pickle(html, _html_pkl)
        logger.info('  write to pickle.')

    # htmlæ–‡å­—åˆ—ã‚’è§£æžã—ã¦ShopItemã«
    results = parse(html)
    df = pd.DataFrame(results, columns=settings.FEED_EXPORT_FIELDS)
    df.to_csv(outfile, index=False, encoding=settings.FEED_EXPORT_ENCODING)


if __name__ == "__main__":
    """
    å¤§åˆ†çœŒã®ã‚µã‚¤ãƒˆã¯SPAãªã®ã§scrapyå˜ä½“ã§ã¯å‡¦ç†ã§ããªã„ã€‚
    splashã‚’å¤§åˆ†ã®ãŸã‚ã ã‘ã«ä½¿ã†ã®ã‚‚ã‚ã‚“ã©ãã•ã‹ã£ãŸã®ã§ã€pyppeteerã§ã‚´ãƒªã‚´ãƒªå®Ÿè£…ã€‚

    usage:
    $ python -m goto_eat_scrapy.scripts.oita
    """
    outfile = '/tmp/44_oita.csv'
    main(outfile)
    logger.info(f'ðŸ‘ success!! > {outfile}')
