import asyncio
import logging
import pathlib

import logzero
import lxml.html
import nest_asyncio
import pandas as pd
from pyppeteer import launch
from pyppeteer.errors import PageError

from goto_eat_scrapy import settings
from goto_eat_scrapy.items import ShopItem

nest_asyncio.apply()


class OitaCrawler:
    """
    å¤§åˆ†çœŒã®ã‚µã‚¤ãƒˆã¯SPAãªã®ã§scrapyå˜ä½“ã§ã¯å‡¦ç†ã§ããªã„ã€‚
    splashã‚’å¤§åˆ†ã®ãŸã‚ã ã‘ã«ä½¿ã†ã®ã‚‚ã‚ã‚“ã©ãã•ã‹ã£ãŸã®ã§ã€pyppeteerã§ã‚´ãƒªã‚´ãƒªå®Ÿè£…ã€‚
    """

    name = "oita"

    LOG_LEVEL = logging.DEBUG
    SLEEP_SEC = 2
    HEADERS = {"User-Agent": settings.USER_AGENT}
    CACHE_PATH = pathlib.Path.cwd() / ".scrapy" / settings.HTTPCACHE_DIR / f"{name}_script"
    CACHE_PATH.mkdir(parents=True, exist_ok=True)

    def _check_pyppeteer(self):
        # @see https://rinoguchi.hatenablog.com/entry/2020/08/09/004925#pyppeteererrorsBrowserError-Browser-closed-unexpectedly
        # Dockerå†…ã§pyppeteerãŒå‹•ã‹ãªã„å ´åˆãŒã‚ã‚‹ã®ã§ã€ãã®ç¢ºèªç”¨
        import os

        from pyppeteer.launcher import Launcher

        cmd: str = " ".join(Launcher({"args": ["--no-sandbox"]}).cmd)
        print(f"cmd: {cmd}")
        os.system(cmd)

    def __init__(self, csvfile=None, logfile=None, with_cache=True):
        # self._check_pyppeteer()
        logger_name = f"logzero_logger_{self.name}"
        if logfile:
            # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºã™(æ¨™æº–å‡ºåŠ›ã«ã¯å‡ºã•ãªã„)
            self.logzero_logger = logzero.setup_logger(
                name=logger_name, logfile=logfile, fileLoglevel=self.LOG_LEVEL, disableStderrLogger=False
            )
        else:
            # æ¨™æº–å‡ºåŠ›ã«å‡ºã™ã®ã¿
            self.logzero_logger = logzero.setup_logger(name=logger_name, level=self.LOG_LEVEL)

        self.csvfile = csvfile
        self.logfile = logfile
        self.with_cache = with_cache

    async def crawl_by_pyppeteer(self):
        browser = await launch(
            {
                "defaultViewport": None,
                "logLevel": logging.INFO,  # æœªæŒ‡å®šã ã¨root loggerã¨åŒã˜ã«ãªã‚‹ã®ã§
                "args": ["--no-sandbox"],  # Dockerå†…ã§å‹•ã‹ã™å ´åˆã«å¿…è¦
                "slowMo": 5,  # é©å®œwaitã‚’é£Ÿã‚ã›ãªã„ã¨ã‚³ã‚±ã‚‹(ï¼Ÿ)ã®ã§
                "headless": True,  # æ‰‹å…ƒã§å‹•ã‹ã—ã¦ã„ã‚‹å ´åˆã¯Falseã«ã™ã‚‹ã¨ã€å®Ÿéš›ã«chroniumãŒå‹•ãã®ã§ã‚ã‹ã‚Šã‚„ã™ã„
            }
        )
        page = await browser.newPage()
        await page.goto("https://oita-gotoeat.com/shop/")

        try:
            # é©å½“ã«waitå…¥ã‚Œã¤ã¤ç„¡é™ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã®ã€Œã‚‚ã£ã¨è¦‹ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’é€£æ‰“
            while True:
                await page.evaluate("""{window.scrollBy(0, document.body.scrollHeight);}""")
                await page.waitFor(self.SLEEP_SEC * 1000)
                await page.click('input[class="more"]')
                self.logzero_logger.debug("  next page...")
        except PageError:
            # FIXME: ã‚¯ã‚½å®Ÿè£…ã€æ¬¡ãƒšãƒ¼ã‚¸ãŒãªããªã£ãŸã‚‰ãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯ãŒã§ããšã€PageErrorãŒraiseã•ã‚Œã‚‹ã®ã§ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
            pass

        html: str = await page.content()
        await browser.close()

        if not html:
            raise Exception("html is Not Found...")

        return html

    def parse(self, html: str):
        """
        lxmlã‚’ä½¿ã£ã¦xpathãƒ™ãƒ¼ã‚¹ã§parse
        """
        results = []
        response = lxml.html.fromstring(html)

        for article in response.xpath('//ul[@class="shop-list cf"]/li[@class="box-sh cf"]'):
            item = ShopItem()
            item["area_name"] = article.xpath('.//div[@class="tag cf"]/p[@class="area"]/span/text()')[0].strip()
            genres = [g.strip() for g in article.xpath('.//div[@class="tag cf"]/p[@class="genre"]/span/text()')]
            item["genre_name"] = "|".join(genres)

            item["shop_name"] = article.xpath('.//p[@class="name"]/text()')[0].strip()
            item["address"] = article.xpath('.//div[@class="first"]/p[@class="add"]/text()')[0].strip()

            tel = article.xpath('.//div[@class="second"]/p[@class="s-call"]/span[@class="shoptel"]/a/text()')
            item["tel"] = tel[0].strip() if tel else None
            official_page = article.xpath('.//div[@class="first"]/p[@class="web"]/a/@href')
            item["official_page"] = official_page[0].strip() if official_page else None

            results.append(item)

        return results

    def crawl(self):
        # ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã¯æ™‚é–“ã‹ã‹ã‚‹ã®ã§(é–‹ç™ºç”¨ã«ã¯)ä¸€å›æˆåŠŸã—ãŸã‚‰å–å¾—ã—ãŸhtmlã‚’pickleã«ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        # ä¿å­˜å…ˆã¯scrapyã®httpcacheã¨åŒã˜å ´æ‰€(settings.HTTPCACHE_DIR)
        cache_file = self.CACHE_PATH / "pyppeteer.pkl"
        if self.with_cache and cache_file.exists():
            self.logzero_logger.debug(f"ğŸ“— load from cache... {cache_file}")
            html = pd.read_pickle(cache_file)
        else:
            self.logzero_logger.info("  crawling ...")
            html = asyncio.get_event_loop().run_until_complete(self.crawl_by_pyppeteer())
            if self.with_cache:
                pd.to_pickle(html, cache_file)
                self.logzero_logger.debug(f"ğŸ’¾  write cache. {cache_file}")

        # htmlæ–‡å­—åˆ—ã‚’è§£æ
        df = pd.DataFrame(self.parse(html), columns=settings.FEED_EXPORT_FIELDS)

        if self.csvfile:
            df.to_csv(self.csvfile, index=False, encoding=settings.FEED_EXPORT_ENCODING)
            self.logzero_logger.info(f"  write csv. {self.csvfile}")
        else:
            self.logzero_logger.info(df)

        if len(df) < 2:
            raise Exception("CSVãŒç©ºã§ã™ã€‚å¤šåˆ†DOMæ§‹é€ ãŒå¤‰ã‚ã£ã¦ã‚‹ã®ã§XPathã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")


if __name__ == "__main__":
    """
    usage:
    $ python -m goto_eat_scrapy.scripts.oita
    """

    crawler = OitaCrawler()
    crawler.crawl()

    # crawler = OitaCrawler(csvfile=pathlib.Path('/tmp/oita.csv'), logfile=pathlib.Path('/tmp/oita.log'), with_cache=True)
    # crawler.crawl()

    print(f"success!!")
