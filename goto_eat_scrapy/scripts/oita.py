import asyncio
from pyppeteer import launch
from pyppeteer.errors import PageError
import lxml.html
import pandas as pd
from logzero import logger
from goto_eat_scrapy import settings
from goto_eat_scrapy.items import ShopItem

async def crawl():
    browser = await launch({
        'defaultViewport': None,
        # é–‹ç™ºä¸­ã¯ä»¥ä¸‹ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã¨å®Ÿéš›ã«ãƒ–ãƒ©ã‚¦ã‚¶ãŒå‹•ãã®ã§ã‚ã‹ã‚Šã‚„ã™ã„
        # 'headless': False,
        # 'slowMo': 5,
    })
    page = await browser.newPage()
    await page.goto('https://oita-gotoeat.com/shop/')

    try:
        # é©å½“ã«waitå…¥ã‚Œã¤ã¤ç„¡é™ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã®ã€Œã‚‚ã£ã¨è¦‹ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’é€£æ‰“
        while True:
            await page.evaluate("""{window.scrollBy(0, document.body.scrollHeight);}""")
            await page.waitFor(1000);
            await page.click('input[class="more"]')
    except PageError:
        # FIXME: ã‚¯ã‚½å®Ÿè£…ã€æ¬¡ãƒšãƒ¼ã‚¸ãŒãªããªã£ãŸã‚‰ãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯ãŒã§ããšã€PageErrorãŒraiseã•ã‚Œã‚‹ã®ã§ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
        pass

    html: str = await page.content()
    await browser.close()

    return html


def parse(html: str):
    """
    lxmlã‚’ä½¿ã£ã¦xpathãƒ™ãƒ¼ã‚¹ã§parse
    """
    results = []
    response = lxml.html.fromstring(html)
    for article in response.xpath('//li[@class="box-sh cf"]'):
        item = ShopItem()
        item['genre_name'] = article.xpath('.//div[@class="tag cf"]/p[@class="genre"]/span/text()')[0].strip()
        item['shop_name'] = article.xpath('.//p[@class="name"]/text()')[0].strip()
        item['address'] = article.xpath('.//div[@class="first"]/p[@class="add"]/text()')[0].strip()
        tel = article.xpath('.//div[@class="second"]/p[@class="s-call"]/span[@class="shoptel"]/a/text()')
        item['tel'] = tel[0].strip() if tel else None

        logger.debug(item)
        results.append(item)

    return results


if __name__ == "__main__":
    """
    å¤§åˆ†çœŒã®ã‚µã‚¤ãƒˆã¯SPAãªã®ã§scrapyå˜ä½“ã ã¨å‡¦ç†ã§ããªã„ã€‚
    splashã‚’å¤§åˆ†ã®ãŸã‚ã ã‘ã«ä½¿ã†ã®ã‚‚ã‚ã‚“ã©ãã•ã‹ã£ãŸã®ã§pyppeteerã§ã‚´ãƒªã‚´ãƒªå®Ÿè£…ã€‚

    usage:
    $ python -m goto_eat_scrapy.scripts.oita
    """
    # ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã¯æ™‚é–“ã‹ã‹ã‚‹ã®ã§ä¸€å›žæˆåŠŸã—ãŸã‚‰pickleã«ã—ã¦ã‚‹
    _html_pkl = "/tmp/44_oita.pkl"
    try:
        logger.info('  load from pickle ...')
        html = pd.read_pickle(_html_pkl)
    except FileNotFoundError:
        logger.info('  crawling ...')
        html = asyncio.get_event_loop().run_until_complete(crawl())
        pd.to_pickle(html, _html_pkl)
        logger.info('  write to pickle.')

    # htmlæ–‡å­—åˆ—ã‚’è§£æžã—ã¦ShopItemã«
    results = parse(html)

    df = pd.DataFrame(results, columns=settings.FEED_EXPORT_FIELDS)
    outfile = '/tmp/44_oita.csv' # ã‚„ã‚‹æ°—ãŒãŠã‚ã‚Šã ã‚ˆ
    df.to_csv(outfile, index=False)

    logger.info(f'ðŸ‘ success!! > {outfile}')
