import logging
import pathlib

import logzero
import lxml.html
import pandas as pd
from playwright.sync_api import sync_playwright

from goto_eat_scrapy import settings
from goto_eat_scrapy.items import ShopItem

name = "oita"

CACHE_PATH = pathlib.Path.cwd() / ".scrapy" / settings.HTTPCACHE_DIR / f"{name}_script"
CACHE_PATH.mkdir(parents=True, exist_ok=True)


def run(playwright, logzero_logger):
    # iPhone11ç›¸å½“(webkit)ã§ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ãŒã€UAã¯æ˜ç¤ºçš„ã«goto-eaterã‚’ç¤ºã™
    browser = playwright.webkit.launch(headless=True)
    device = playwright.devices["iPhone 11"]
    device["user_agent"] = settings.USER_AGENT
    context = browser.new_context(**device)

    page = context.new_page()
    page.goto("https://oita-gotoeat.com/shop/")

    try:
        # ç„¡é™ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã®ã€Œã‚‚ã£ã¨è¦‹ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’é€£æ‰“
        while True:
            page.evaluate("""{window.scrollBy(0, document.body.scrollHeight);}""")
            page.click('input[class="more"]')
            logzero_logger.debug("next page...")
    except Exception:
        pass

    html = page.content()
    context.close()
    browser.close()

    if not html:
        raise Exception("html is Not Found...")

    return html


def parse(html: str):
    """
    lxmlã‚’ä½¿ã£ã¦xpathãƒ™ãƒ¼ã‚¹ã§parse
    """
    results = []
    response = lxml.html.fromstring(html)

    for article in response.xpath('//ul[@class="shop-list cf"]/li[@class="box-sh cf"]'):
        item = ShopItem()
        item["area_name"] = article.xpath('.//div[@class="tag cf"]/p[@class="area"]/span/text()')[0].strip()
        genres = [g.strip() for g in article.xpath('.//div[@class="tag cf"]/p[@class="genre"]/span/text()')]
        item["genre_name"] = "|".join(genres)

        item["shop_name"] = article.xpath('.//p[@class="name"]/text()')[0].strip()
        item["address"] = article.xpath('.//div[@class="first"]/p[@class="add"]/text()')[0].strip()

        tel = article.xpath('.//div[@class="second"]/p[@class="s-call"]/span[@class="shoptel"]/a/text()')
        item["tel"] = tel[0].strip() if tel else None
        official_page = article.xpath('.//div[@class="first"]/p[@class="web"]/a/@href')
        item["official_page"] = official_page[0].strip() if official_page else None

        results.append(item)

    return results


def crawl(csvfile=None, logfile=None, with_cache=True):
    if logfile:
        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®å‡ºåŠ›
        logzero_logger = logzero.setup_logger(
            name=name,
            logfile=logfile,
            level=logging.INFO,
            fileLoglevel=logging.DEBUG,
            # formatter=None,
            disableStderrLogger=False,
        )
    else:
        # æ¨™æº–å‡ºåŠ›(é–‹ç™ºç”¨)
        logzero_logger = logzero.setup_logger(name=name, level=logging.DEBUG, disableStderrLogger=False)

    # ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã¯æ™‚é–“ã‹ã‹ã‚‹ã®ã§(é–‹ç™ºç”¨ã«ã¯)ä¸€å›æˆåŠŸã—ãŸã‚‰å–å¾—ã—ãŸhtmlã‚’pickleã«ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    # ä¿å­˜å…ˆã¯scrapyã®httpcacheã¨åŒã˜å ´æ‰€(settings.HTTPCACHE_DIR)
    cache_file = CACHE_PATH / "playwright.pkl"
    if with_cache and cache_file.exists():
        logzero_logger.debug(f"ğŸ“— load from cache... {cache_file}")
        html = pd.read_pickle(cache_file)
    else:
        logzero_logger.info("  crawling ...")
        with sync_playwright() as playwright:
            html = run(playwright, logzero_logger)
        if with_cache:
            pd.to_pickle(html, cache_file)
            logzero_logger.debug(f"ğŸ’¾  write cache. {cache_file}")

    # htmlæ–‡å­—åˆ—ã‚’è§£æ
    df = pd.DataFrame(parse(html), columns=settings.FEED_EXPORT_FIELDS)

    if csvfile:
        df.to_csv(csvfile, index=False, encoding=settings.FEED_EXPORT_ENCODING)
        logzero_logger.info(f"  write csv. {csvfile}")
    else:
        logzero_logger.info(df)

    if len(df) < 2:
        raise Exception("CSVãŒç©ºã§ã™ã€‚å¤šåˆ†DOMæ§‹é€ ãŒå¤‰ã‚ã£ã¦ã‚‹ã®ã§XPathã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")


if __name__ == "__main__":
    """
    usage:
    $ python -m goto_eat_scrapy.scripts.oita
    """

    crawl()

    print(f"success!!")
