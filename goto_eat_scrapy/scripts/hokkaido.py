import logging
import pathlib
import time

import logzero
import lxml
import pandas as pd
import requests
from logzero import logger

from goto_eat_scrapy import settings
from goto_eat_scrapy.items import ShopItem


class HokkaidoCrawler:
    """
    åŒ—æµ·é“ã®ã‚µã‚¤ãƒˆã¯ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å…±æœ‰ã—ã¦ã„ã‚‹ãŸã‚ã€æ¤œç´¢çµæœã®URLãŒä¸€æ„ã«ãªã‚‰ãšã€ä¸¦åˆ—ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã¨çµæœãŒæ··ã–ã£ã¦ã—ã¾ã†ã€‚
    è‰²ã€…ã¨è©¦è¡ŒéŒ¯èª¤ã—ãŸãŒScrapyã§ã¯ã‚·ãƒ³ãƒ—ãƒ«ã«å‡¦ç†ã§ããªã‹ã£ãŸãŸã‚ã€æ„šç›´ã«requests + lxmlã§ã‚´ãƒªã‚´ãƒªã¨å®Ÿè£…ã—ãŸâ€¦
    """

    name = "hokkaido"

    LOG_LEVEL = logging.INFO
    SLEEP_SEC = 2
    HEADERS = {"User-Agent": settings.USER_AGENT}
    CACHE_PATH = pathlib.Path.cwd() / ".scrapy" / settings.HTTPCACHE_DIR / f"{name}_script"
    CACHE_PATH.mkdir(parents=True, exist_ok=True)

    def __init__(self, csvfile=None, logfile=None, with_cache=True):
        logger_name = f"logzero_logger_{self.name}"
        if logfile:
            # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºã™(æ¨™æº–å‡ºåŠ›ã«ã¯å‡ºã•ãªã„)
            self.logzero_logger = logzero.setup_logger(
                name=logger_name,
                logfile=logfile,
                fileLoglevel=self.LOG_LEVEL,
                disableStderrLogger=True,
            )
        else:
            # æ¨™æº–å‡ºåŠ›ã«å‡ºã™ã®ã¿
            self.logzero_logger = logzero.setup_logger(name=logger_name, level=self.LOG_LEVEL)

        self.csvfile = csvfile
        self.logfile = logfile
        self.session = requests.Session()
        self.token = self.show_search_page_and_get_token()
        self.with_cache = with_cache

    def show_search_page_and_get_token(self):
        """
        ã€Œå–æ‰±åº—ãƒªã‚¹ãƒˆã€ã®åˆæœŸè¡¨ç¤º(GET)ã‚’è¡Œã„ã€token(csrfå¯¾ç­–ç”¨ï¼Ÿ)ã‚’å–å¾—
        """
        r = self.session.get("https://gotoeat-hokkaido.jp/general/particStores", headers=self.HEADERS)
        r.raise_for_status()  # ãƒªãƒˆãƒ©ã‚¤ãªã—ã€‚ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ 40x/50x ã¯ä¾‹å¤–ã§å³çµ‚äº†ã¨ã™ã‚‹

        html = lxml.html.fromstring(r.content)
        token = html.xpath('//p[@class="buttons"]/input[@name="_token"]/@value')[0]

        return token

    def post_search(self, area):
        """
        ã‚¨ãƒªã‚¢ã‚’é¸æŠã—ã¦ã€Œæ¤œç´¢ã™ã‚‹ã€ãƒœã‚¿ãƒ³æŠ¼ä¸‹(POST)
        """
        token = self.token
        self.logzero_logger.info(f"POST ... (_token={token}, area={area})")
        params = {
            "store_area": area,
            "store_address1": "",
            "division1_id": "",
            "store_name": "",
            "_token": token,
        }

        # FIXME: æœ¬æ¥ã¯ã“ã£ã¡ã‚‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ã¹ããªã‚“ã ã‘ã©æ‰‹ã‚’æŠœã„ã¦ã„ã‚‹
        # å„ã‚¨ãƒªã‚¢æœ€åˆã®1å›ã ã‘ãªã®ã§ã‚†ã‚‹ã—ã¦â€¦
        time.sleep(self.SLEEP_SEC)
        r = self.session.post("https://gotoeat-hokkaido.jp/general/particStores/search", params, headers=self.HEADERS)
        r.raise_for_status()

        html = lxml.html.fromstring(r.content)
        return self.parse(html, area_name=area)

    def get_page(self, url, area):
        """
        æ¤œç´¢çµæœã‚’ãƒšãƒ¼ã‚¸ãƒ³ã‚°(GET /?page=xx)
        """
        self.logzero_logger.info(f"GET {url}...")

        # requestã«å¯¾ã—ã¦ã®pickleã‚’ç”¨ã„ãŸç°¡æ˜“ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        # ä¿å­˜å…ˆã¯scrapyã®httpcacheã¨åŒã˜å ´æ‰€(settings.HTTPCACHE_DIR)
        cache_file = self.CACHE_PATH / "{}_{}.pkl".format(url.replace("/", "_").replace("?", "_"), area)
        if self.with_cache and cache_file.exists():
            self.logzero_logger.debug(f"ğŸ“— load from cache... {cache_file}")
            r = pd.read_pickle(cache_file)
        else:
            time.sleep(self.SLEEP_SEC)
            r = self.session.get(url, headers=self.HEADERS)
            r.raise_for_status()
            if self.with_cache:
                pd.to_pickle(r, cache_file)
                self.logzero_logger.debug(f"ğŸ’¾ write cache. {cache_file}")

        response = lxml.html.fromstring(r.content)
        return self.parse(response, area_name=area)

    def parse(self, response, area_name):
        """
        htmlã‚’è§£æ
        """
        result = []
        for article in response.xpath('//div[@id="contents"]/div[@class="results"]/ul/li'):
            item = ShopItem()
            item["area_name"] = area_name
            item["shop_name"] = article.xpath('.//div[@class="left"]/h4[@class="results-tit"]/text()')[0].strip()
            item["address"] = article.xpath('.//div[@class="left"]/p[@class="results-txt01"]/text()')[0].strip()
            item["genre_name"] = article.xpath('.//div[@class="right"]/p[@class="results-txt02"]/text()')[0].strip()
            tel = article.xpath('.//div[@class="right"]/p[@class="results-txt03"]/text()')
            item["tel"] = tel[0].strip() if tel else None

            result.append(item)

        next_page = response.xpath('//ul[@role="navigation"]/li/a[@rel="next"]/@href')
        return result, next_page

    def crawl(self):
        results = []
        for area in ["é“å¤®", "é“åŒ—", "é“å—", "é“æ±"]:
            # æ¤œç´¢çµæœã®1Pç›®(POST)
            result, next_page = self.post_search(area)
            results += result
            # æ¤œç´¢çµæœã®2Pä»¥é™(GET)
            while next_page:
                result, next_page = self.get_page(url=next_page[0], area=area)
                results += result

        df = pd.DataFrame(results, columns=settings.FEED_EXPORT_FIELDS)
        if self.csvfile:
            df.to_csv(self.csvfile, index=False, encoding=settings.FEED_EXPORT_ENCODING)
            self.logzero_logger.info(f"  write csv. {self.csvfile}")
        else:
            self.logzero_logger.info(df)


if __name__ == "__main__":
    """
    usage:
    $ python -m goto_eat_scrapy.scripts.hokkaido
    """
    crawler = HokkaidoCrawler()
    crawler.crawl()

    # crawler = HokkaidoCrawler(csvfile=pathlib.Path('/tmp/hokkaido.csv'), logfile=pathlib.Path('/tmp/hokkaido.log'), with_cache=True)
    # crawler.crawl()

    print(f"success!!")
