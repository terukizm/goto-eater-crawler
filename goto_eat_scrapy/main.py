from scrapy.utils.project import get_project_settings
from scrapy.crawler import CrawlerProcess
from scrapy.utils.log import configure_logging
import pathlib
import pandas as pd
import multiprocessing
from logzero import logger
from goto_eat_scrapy.scripts import oita
from goto_eat_scrapy.scripts import hokkaido


class Main():
    def __init__(self, base: pathlib.Path):
        self.csv_dir = base / 'csvs'
        self.log_dir = base / 'logs'
        self.csv_dir.mkdir(parents=True, exist_ok=True)
        self.log_dir.mkdir(parents=True, exist_ok=True)

    def run_spiders(self):
        logger.info('Scrapyã®Spiderã‚’å®Ÿè¡Œ ... ')

        settings = get_project_settings()
        settings.set('FEED_FORMAT', 'csv')
        settings.set('FEED_URI', str(self.csv_dir / '%(name)s.csv'))  # @see https://docs.scrapy.org/en/latest/topics/feed-exports.html#storage-uri-parameters
        settings.set('LOG_FILE', str(self.log_dir / '_scrapy.log'))   # scrapyã®ãƒ­ã‚°(request/httpcacheå‘¨ã‚Šã®åˆ‡ã‚Šåˆ†ã‘ç”¨)

        process = CrawlerProcess(settings)
        spiders = process.spiders.list()
        # å˜ä½“å‹•ä½œç¢ºèªç”¨
        spiders = [
            # 'aichi',
            # 'aomori',
            # 'akita',
            # 'ehime',
            # 'fukui',
            # 'fukuoka',
            # 'fukushima',
            # 'gifu',
            # 'gunma',
            # 'hiroshima',
            # 'hyogo',
            # 'ibaraki',
            # 'ishikawa',
            # 'iwate',
            # 'kagawa',
            'kagoshima',
            # 'kochi',
            # 'kumamoto',
            # 'kyoto',
            # 'mie',
            # 'miyagi',
            # 'miyazaki',
            # 'nagano',
            # 'nagasaki',
            # 'nara',
            # 'niigata',
            # 'okayama',
            # 'okinawa',
            # 'osaka',
            # 'saga',
            # 'saitama',
            # 'shimane',
            # 'shizuoka',
            # 'tochigi',
            # 'tokushima',  ### ã€Œâ€»æœ¬ã‚µã‚¤ãƒˆã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ç„¡æ–­è»¢è¼‰ã‚’ç¦ã˜ã¾ã™ã€‚ã€ã¨ã„ã†ä¸€æ–‡ãŒã‚ã‚‹ã®ã§skip
            # 'tokyo',      ### ä¼æ¥­ã‚µã‚¤ãƒˆ(ãâ—‹ãªã³)ãªä¸Šã€ä»¶æ•°ãŒå¤šãã¦ã€ã‹ã¤è©³ç´°ãƒšãƒ¼ã‚¸ã¾ã§è¦‹ãªã„ã¨ã„ã‘ãªã„
            # 'tottori',
            # 'toyama',
            # 'wakayama',
            # 'yamagata',
            # 'yamaguchi',
            # 'yamanashi',
            #
            # MEMO: æœªå¯¾å¿œ
            ### å…¬å¼ã§åœ°å›³ã‚¢ãƒ—ãƒªãŒæä¾›ã•ã‚Œã¦ãŠã‚Šã€latlngè‡ªä½“ãŒgoogle mapsã‹ã‚‰å–ã£ã¦ãã¦ã‚‹å€¤ã®å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚
            # 'chiba',
            # 'kanagawa',
            # 'shiga'
        ]

        for spider in spiders:
            logger.info(f'[ {spider} ] start ...')

            # log, csvã¯ä¸Šæ›¸ã(ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãªã—)
            # MEMO: 2020/10ãƒªãƒªãƒ¼ã‚¹ã®v2.4.0ä»¥é™ã§è¿½åŠ ã•ã‚ŒãŸoverwriteã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§CSVã‚’ä¸Šæ›¸ãã§ãã‚Œã°ã‚ˆã„ã®ã ãŒã€
            # ã‚¤ãƒã‚¤ãƒä½¿ã„æ–¹ãŒã‚ã‹ã‚‰ãªã„ã®ã§ã€å¤ã„CSVã‚’å‰Šé™¤ã™ã‚‹ã“ã¨ã§çµæœãŒè¿½è¨˜ã•ã‚Œãªã„ã‚ˆã†ã«ã—ã¦ã„ã‚‹
            csvfile = self.csv_dir / f'{spider}.csv'
            logfile = self.log_dir / f'{spider}.log'
            csvfile.unlink(missing_ok=True)
            logfile.unlink(missing_ok=True)

            process.crawl(spider, logfile=logfile)
            logger.info(f'[ {spider} ]  end  ...')

        process.start()
        logger.info('all complete!!! ')


    def run_scripts(self):
        logger.info('Scrapyä»¥å¤–ã§æ›¸ã‹ã‚ŒãŸã‚¯ãƒ­ãƒ¼ãƒ©ã‚’å®Ÿè¡Œ...')
        p1 = multiprocessing.Process(name="åŒ—æµ·é“", target=hokkaido.main, args=(self.csv_dir / 'hokkaido.csv', ))
        p2 = multiprocessing.Process(name="å¤§åˆ†çœŒ", target=oita.main, args=(self.csv_dir / 'oita.csv', ))
        p1.start()
        p2.start()
        p1.join()
        p2.join()
        logger.info('... å®Œäº†')


    def sort_csv(self):
        logger.info('å‡ºåŠ›ã•ã‚ŒãŸCSVã‚’ã‚½ãƒ¼ãƒˆ...')
        for csv in list(self.csv_dir.glob('*.csv')):
            # å‡ºåŠ›ã•ã‚ŒãŸCSVã‚’åº—èˆ—åã€ä½æ‰€ã€(ã‚¸ãƒ£ãƒ³ãƒ«å)ã§ã‚½ãƒ¼ãƒˆã—ãŸå¾Œã€ä¸Šæ›¸ã
            df = pd.read_csv(csv).sort_values(['shop_name', 'address', 'genre_name'])
            df.to_csv(csv, index=False)
            # df.to_csv(csv.parent / (csv.name + '.csv.sorted'), index=False)  # åˆ¥åä¿å­˜ã™ã‚‹å ´åˆ
        logger.info('... å®Œäº†')


if __name__ == "__main__":
    # usage:
    # $ python -m goto_eat_scrapy.main

    # TODO: get from args
    base = pathlib.Path.cwd() / 'data'

    main = Main(base)
    main.run_scripts()
    # main.run_spiders()
    # main.sort_csv()

    logger.info(f'ğŸ‘ çµ‚äº†')
