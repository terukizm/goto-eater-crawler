from scrapy.utils.project import get_project_settings
from scrapy.crawler import CrawlerProcess
from scrapy.utils.log import configure_logging
import pathlib
import pandas as pd
import multiprocessing
from goto_eat_scrapy.scripts import oita
from goto_eat_scrapy.scripts import hokkaido
from logzero import logger
import datetime

def run_spiders(base='result/csv'):
    logger.info('... Scrapyã®Spiderã‚’å®Ÿè¡Œ ... ')
    settings = get_project_settings()
    settings.set('FEED_URI', f'{base}/%(name)s.csv')
    settings.set('FEED_FORMAT', 'csv')
    settings.set('LOG_FILE', 'tmp/scrapy.log')  # scrapyã®ãƒ­ã‚°å‡ºåŠ›(request/httpcacheå‘¨ã‚Šã®åˆ‡ã‚Šåˆ†ã‘ç”¨)

    process = CrawlerProcess(settings)
    spiders = process.spiders.list()
    # å˜ä½“å‹•ä½œç¢ºèª
    spiders = [
        # 'aichi',
        # 'aomori',
        # 'akita',
        # 'fukui',
        # 'fukuoka',
        # 'fukushima',
        # 'gifu',
        # 'gunma',
        # 'hiroshima',
        # 'hyogo',
        # 'ibaraki',
        # 'ishikawa',
        # 'iwate',
        # 'kagawa',
        # 'kagoshima',
        # 'kochi',
        # 'kumamoto',
        # 'kyoto',
        # 'mie',
        # 'miyazaki',
        # 'nagano',
        # 'nagasaki',
        # 'nara',
        # 'niigata',
        'okinawa',
        'osaka',
        # 'saga',
        # 'saitama',
        # 'shimane',
        # 'shizuoka',
        # 'tochigi',
        '',
        '',
        '',
        '',
    ]

    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    for spider in spiders:
        # MEMO: 2020/10ãƒªãƒªãƒ¼ã‚¹ã®v2.4.0ã§è¿½åŠ ã•ã‚ŒãŸoverwriteã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§CSVã‚’ä¸Šæ›¸ãã§ãã‚Œã°ã‚ˆã„ã®ã ãŒã€
        # ã‚¤ãƒã‚¤ãƒä½¿ã„æ–¹ãŒã‚ã‹ã‚‰ãªã„ã®ã§å¤ã„CSVã«è¿½è¨˜ã•ã‚Œãªã„ã‚ˆã†ã«æ¶ˆã—ã¦ã„ã‚‹
        logger.info(f'[ {spider} ] start ...')
        (pathlib.Path.cwd() / f'{base}/{spider}.csv' ).unlink(missing_ok=True)
        process.crawl(spider, logfile=f'{base}/logs/{spider}_{timestamp}.log')
        logger.info(f'[ {spider} ]  end ...')

    process.start()
    logger.info('all complete!!! ')


def run_scripts(base='result/csv'):
    logger.info('... Scrapyä»¥å¤–ã§æ›¸ã‹ã‚ŒãŸã‚¯ãƒ­ãƒ¼ãƒ©ã‚’å®Ÿè¡Œ ... ')
    p1 = multiprocessing.Process(name="åŒ—æµ·é“", target=hokkaido.main, args=(f'{base}/hokkaido.csv', ))
    p2 = multiprocessing.Process(name="å¤§åˆ†çœŒ", target=oita.main, args=(f'{base}/oita.csv', ))
    p1.start()
    p2.start()
    p1.join()
    p2.join()
    logger.info('... å®Œäº†!!')


def sort_csv(base='result/csv'):
    logger.info('... å‡ºåŠ›ã•ã‚ŒãŸCSVã‚’ã‚½ãƒ¼ãƒˆ... ')
    target = pathlib.Path.cwd() / base
    for csv in list(target.glob('*.csv')):
        df = pd.read_csv(csv).sort_values(['shop_name', 'address', 'genre_name'])
        df.to_csv(csv, index=False) # ä¸Šæ›¸ã
        # df.to_csv(csv.parent / (csv.name + '.csv.sorted'), index=False)  # åˆ¥åä¿å­˜
    logger.info('... å®Œäº†!!')


if __name__ == "__main__":
    # TODO: use args
    run_spiders()
    # run_scripts()
    # sort_csv()

    logger.info('ğŸ‘ å®Œäº†')
