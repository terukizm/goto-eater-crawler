from scrapy.utils.project import get_project_settings
from scrapy.crawler import CrawlerProcess
from scrapy.utils.log import configure_logging
import pathlib
import pandas as pd
import argparse
from logzero import logger
from goto_eat_scrapy.scripts.oita import OitaCrawler
from goto_eat_scrapy.scripts.hokkaido import HokkaidoCrawler

class Main():
    def __init__(self, base: pathlib.Path, target=None):
        self.csv_dir = base / 'csvs'
        self.log_dir = base / 'logs'
        self.csv_dir.mkdir(parents=True, exist_ok=True)
        self.log_dir.mkdir(parents=True, exist_ok=True)

        settings = get_project_settings()
        settings.set('FEED_FORMAT', 'csv')
        settings.set('FEED_URI', str(self.csv_dir / '%(name)s.csv'))  # @see https://docs.scrapy.org/en/latest/topics/feed-exports.html#storage-uri-parameters
        settings.set('LOG_FILE', str(self.log_dir / '_scrapy.log'))   # scrapyã®ãƒ­ã‚°(request/httpcacheå‘¨ã‚Šã®åˆ‡ã‚Šåˆ†ã‘ç”¨)
        self.settings = settings

    def run(self, target):
        targets = target.split(',') if target else None
        if not targets:  # all
            # ç‰¹å®šã®éƒ½é“åºœçœŒã‚’é™¤ã„ã¦ä¸€æ‹¬å®Ÿè¡Œ
            ignores = [
                'tokyo',     # ä¼æ¥­ã‚µã‚¤ãƒˆã§ã‚ã‚Šã€ã‹ã¤ä»¶æ•°ãŒå¤šãã€è©³ç´°ãƒšãƒ¼ã‚¸ã¾ã§è¦‹ã‚‹å¿…è¦ãŒã‚ã‚Šã€ã‚¢ã‚¯ã‚»ã‚¹ãŒå¤šããªã£ã¦ã—ã¾ã†ãŸã‚
                'tokushima', # ã€Œâ€»æœ¬ã‚µã‚¤ãƒˆã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ç„¡æ–­è»¢è¼‰ã‚’ç¦ã˜ã¾ã™ã€‚ã€ã¨ã„ã†ä¸€æ–‡ãŒã‚ã‚‹ãŸã‚ (2020/12/09)
                ### ä»¥ä¸‹ã¯å…¬å¼ã§åœ°å›³ã‚¢ãƒ—ãƒªãŒæä¾›ã•ã‚Œã¦ãŠã‚Šã€latlngè‡ªä½“ãŒgoogle mapsã‹ã‚‰å–ã£ã¦ãã¦ã„ã‚‹å€¤ã®å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚
                'chiba',
                'kanagawa',
                'shiga',
            ]
            process = CrawlerProcess(self.settings)
            targets = [ x for x in process.spiders.list() if not x in ignores ]
            targets += ['hokkaido', 'oita']

        if 'hokkaido' in targets:
            self.run_hokkaido()
            targets.remove('hokkaido')
        if 'oita' in targets:
            self.run_oita()
            targets.remove('oita')
        if targets:
            self.run_spiders(spiders=targets)

        logger.info('completed!! ')

    def run_spiders(self, spiders: list):
        logger.info('Scrapyã®Spiderã‚’å®Ÿè¡Œ ... ')
        process = CrawlerProcess(self.settings)
        for spider in spiders:
            logger.info(f'[ {spider} ] start ...')

            # log, csvã¯ä¸Šæ›¸ã(ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãªã—)
            # MEMO: 2020/10ãƒªãƒªãƒ¼ã‚¹ã®v2.4.0ä»¥é™ã§è¿½åŠ ã•ã‚ŒãŸoverwriteã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§CSVã‚’ä¸Šæ›¸ãã§ãã‚Œã°ã‚ˆã„ã®ã ãŒã€
            # ã‚¤ãƒã‚¤ãƒä½¿ã„æ–¹ãŒã‚ã‹ã‚‰ãªã„ã®ã§ã€å¤ã„CSVã‚’å‰Šé™¤ã™ã‚‹ã“ã¨ã§çµæœãŒè¿½è¨˜ã•ã‚Œãªã„ã‚ˆã†ã«ã—ã¦ã„ã‚‹
            csvfile = self.csv_dir / f'{spider}.csv'
            logfile = self.log_dir / f'{spider}.log'
            csvfile.unlink(missing_ok=True)
            logfile.unlink(missing_ok=True)

            process.crawl(spider, logfile=logfile)
            logger.info(f'[ {spider} ]  end  ...')

        process.start()

    def run_hokkaido(self):
        # FIXME: multiprocessing.Processã§ã‚„ã£ã¤ã‘ä¸¦è¡Œå‡¦ç†ã—ã¦ã„ãŸãŒã€loggingãŒã†ã¾ãå‡ºãªã„(loggerãŒå·®ã—æ›¿ãˆã‚‰ã‚Œã¦ã—ã¾ã†ï¼Ÿ)ã®ã§
        # ã‚´ãƒªã‚´ãƒªæ›¸ã„ã¦ã„ã‚‹ã€‚åŒ—æµ·é“ã€å¤§åˆ†çœŒã¨ã‚‚ã«å¤šå°‘ä»¶æ•°ã¯ã‚ã‚‹ãŒã€ã¾ã‚å‡¦ç†ã—ãã‚Œãªã„ã»ã©ã®ä»¶æ•°ã§ã¯ãªã„ã®ã§â€¦
        target = 'hokkaido'
        logger.info(f'[ {target} ] start ...')
        csvfile = self.csv_dir / f'{target}.csv'
        logfile = self.log_dir / f'{target}.log'
        csvfile.unlink(missing_ok=True)
        logfile.unlink(missing_ok=True)
        c1 = HokkaidoCrawler(csvfile, logfile)
        c1.crawl()
        logger.info(f'[ {target} ]  end  ...')

    def run_oita(self):
        # TODO: åŒ—æµ·é“ãƒ»å¤§åˆ†çœŒã®ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°
        target = 'oita'
        logger.info(f'[ {target} ] start ...')
        csvfile = self.csv_dir / f'{target}.csv'
        logfile = self.log_dir / f'{target}.log'
        csvfile.unlink(missing_ok=True)
        logfile.unlink(missing_ok=True)
        c2 = OitaCrawler(csvfile, logfile)
        c2.crawl()
        logger.info(f'[ {target} ]  end  ...')

    def sort_csv(self):
        logger.info('å‡ºåŠ›ã•ã‚ŒãŸCSVã‚’ã‚½ãƒ¼ãƒˆ...')
        for csv in list(self.csv_dir.glob('*.csv')):
            # å‡ºåŠ›ã•ã‚ŒãŸCSVã‚’åº—èˆ—åã€ä½æ‰€ã€(+ã‚¸ãƒ£ãƒ³ãƒ«å)ã§ã‚½ãƒ¼ãƒˆã—ãŸå¾Œã€ä¸Šæ›¸ã
            df = pd.read_csv(csv).sort_values(['shop_name', 'address', 'genre_name'])
            df.to_csv(csv, index=False)
            # df.to_csv(csv.parent / (csv.name + '.csv.sorted'), index=False)  # åˆ¥åä¿å­˜ã™ã‚‹å ´åˆ
        logger.info('... å®Œäº†')


if __name__ == "__main__":
    # usage:
    # $ python -m goto_eat_scrapy.main
    # $ python -m goto_eat_scrapy.main --target tochigi,oita,gunma

    # TODO: ã¾ã¨ã‚‚ã«ã‚ªãƒ—ã‚·ãƒ§ãƒ³å¼•æ•°ã‚’ã‚„ã‚‹
    parser = argparse.ArgumentParser(description='goto-eat-crawler')
    parser.add_argument('--basedir', help='ä¾‹: data')
    parser.add_argument('--target', help='ä¾‹: tochigi,gunma')
    args = parser.parse_args()

    base = pathlib.Path(args.basedir) if args.basedir else pathlib.Path.cwd() / 'data'
    main = Main(base)
    main.run(args.target)
    main.sort_csv()

    logger.info(f'ğŸ‘ çµ‚äº†')
